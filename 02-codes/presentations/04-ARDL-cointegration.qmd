---
title: "ARDL et cointégration"
author: "Romain CAPLIEZ"
format: html
editor: visual
toc: TRUE
---

# Théorie

Comme vu au cours du chapitre 3, une série temporelle peut être expliquée par ses propres valeurs passées et les chocs quelle a subi au cours du passé. Cependant, compter uniquement sur l'information disponible par cette série peut être limité. Ajouter de l'information supplémentaire peut être utile afin d'étudier des liens de causalité ou bien afin d'améliorer la qualité de la prévision effectuée.

Il est possible d'enrichir les processus autoprojectifs en intégrant des informations apportées par des variables exogènes.

## Hypothèses classiques des MCO en séries temporelles

Tout comme pour les données en coupe transversales, il existe des hypothèses permettant d'assurer que les estimateurs MCO en séries temporelles soient sans biais et de variance minimale tout en permettant de mener une analyse d'inférence.

-   **Hypothèse TS.1** **:** **Linéarité des paramètres**

Le processus stochastique $\{(x_{t,1}, x_{t,2}, \dots, x_{t,k}, y_t)\}$ : $t=1, 2, \dots, n$ suit un modèle linéaire dans ses paramètres :

$$
y_t = \alpha + \beta_1 X_{1t} + \dots + \beta_k X_{kt} + u_t
$$

Où $u_t$ correspond à la série des erreurs et $n$ le nombre de périodes temps (observations) de l'échantillon.

-   **Hypothèse TS.2 : Absence de colinéarité parfaite**

Dans l'échantillon (et donc dans le processus temporel sous-jacent), aucune variable explicative n'est une constante ou une combinaison linéaire parfaite d'autres variables explicatives.

Cette hypothèse autorise une certaine corrélation entre les variables explicatives tant que celles-ci ne sont pas parfaitement corrélées dans l'échantillon.

Si cette hypothèse n'est pas vérifiée le modèle n'est pas identifiable.

-   **Hypothèse TS.3 : Espérance conditionnelle nulle**

Pour chaque $t$, l'espérance mathématique du terme d'erreur $u_t$ compte tenu des variables explicatives pour toutes les périodes est égale à 0.

$$
E(u_t|\mathbf{X}) = 0, \hspace{0.2cm} t = 1, 2, \dots, n
$$

Cette hypothèse indique que l'erreur à l'instant $t$ ne doit pas être corrélée de quelque manière que ce soit avec les variables explicatives pour chaque période de temps. Les variables doivent être strictement exogènes. Ce qui est une hypothèse plus stricte que la seule exogénéité contemporaine $E(u_t|\mathbf{X}_t) = 0, \hspace{0.2cm} t = 1, 2, \dots, n$.

Sous les hypothèses TS.1 à TS.3 les estimateurs MCO en série temporelle sont sans biais.

-   **Hypothèse TS.4 : Homoscédasticité**

Conditionnellement à $X$, la variance de $u_t$ est la même pour tout $t$ : $Var(u_t | X) = Var(u_t) = \sigma^2 \text{ , } t = 1, 2, \dots, n$. Cela signifie que la variance conditionnelle des erreurs ne doit pas dépendre des variables explicatives (il suffit pour cela que $u_t$ et $X$ soient indépendants) et que la variance des erreurs soit constante au cours du temps.

-   **Hypothèse TS.5 : Absence d'autocorrélation**

Conditionnellement à $X$, les erreurs à deux périodes de temps différentes ne sont pas corrélées entre elles: $Corr(u_t, u_s) = 0 \hspace{0.2cm} \forall \hspace{0.2cm} t \neq s$. Si cette propriété n'est pas vérifiée, on dit que les erreurs souffrent d'autocorrélation ou de corrélation sérielle car elles sont corrélées dans le temps. L'autocorrélation implique que si à une période $t$ le niveau réel de la variable est pus élevé que la niveau attendu, alors il y a de plus fortes chances que ce soit également le cas dans la période suivante.

Sous les hypothèses TS.4 à TS.5, les erreurs sont dites sphériques. Dans ce cas, la variance des estimateurs MCO est minimale.

**Sous les hypothèses TS.1 à TS.5, les estimateurs MCO sont les meilleurs estimateurs linéaires possibles sans biais, conditionnellement à** $X$**.**

-   **Hypothèse TS.6 : Normalité**

Les erreurs $u_t$ sont indépendantes de $X$ et indépendamment et identiquement distribuées selon une loi normale $(0, \sigma^2)$. Cette hypothèse implique les hypothèses TS.3, TS.4, TS.5 mais est plus forte de par les hypothèses d'indépendance et de normalité.

Sous les hypothèses TS.1 à TS.6, les estimateurs des MCO sont normalement distribués conditionnellement à $X$. De plus, sous l'hypothèse nulle, chaque statistique $t$ suit une distribution de Student, et chaque statistique $F$ suit une loi de Fisher.

## Propriétés asymptotiques des MCO

Les hypothèses classiques des MC0 en séries temporelles peuvent être difficiles à remplir. En effet, elles imposent généralement que ces hypothèses soient vérifiées pour toutes les périodes de temps simultanément ce qui est difficile à prouver. Cependant, grâce aux séries dites faiblement dépendantes (stationnaires), nous pouvons assouplir ces hypothèses et utiliser les propriétés asymptotiques des MCO.

-   **Hypothèse TS.1' : Linéarité et faible dépendance**

Le processus stochastique $\{(x_{t,1}, x_{t,2}, \dots, x_{t,k}, y_t)\}$ : $t=1, 2, \dots, n$ suit un modèle linéaire dans ses paramètres :

$$ y_t = \alpha + \beta_1 X_{1t} + \dots + \beta_k X_{kt} + u_t $$

Où $u_t$ correspond à la série des erreurs et $n$ le nombre de périodes temps (observations) de l'échantillon. Ce processus est un processus stationnaire et de faible dépendance. La Loi des Grands Nombres et le Théorème Central Limite s'appliquent aux moyennes de l'échantillon.

La stationnarité de la série n'est pas critique pour les propriétés asymptotiques des MCO (il faut une forme de stabilité dans les temps des relations). La restriction additionnelle majeure de cette hypothèse est l'hypothèse de faible dépendance qui implique que $X_t$ et $X_{t+h}$ sont "presque indépendants" lorsque $h$ augmente.

-   **Hypothèse TS.2' : Absence de parfaite colinéarité**

Cette hypothèse est la même que l'hypothèse TS.2 :

Dans l'échantillon (et donc dans le processus temporel sous-jacent), aucune variable explicative n'est une constante ou une combinaison linéaire parfaite d'autres variables explicatives.

Cette hypothèse autorise une certaine corrélation entre les variables explicatives tant que celles-ci ne sont pas parfaitement corrélées dans l'échantillon.

Si cette hypothèse n'est pas vérifiée le modèle n'est pas identifiable.

-   **Hypothèse TS.3' : Moyenne conditionnelle nulle**

Les variables explicatives sont simultanément exogènes : $E(u_t|X_t) = 0$.

Par stationnarité, on suppose que si l'exogénéité est vérifiée pour une période de temps, cela est vrai aussi pour toutes les périodes de temps. Supprimer la stationnarité nous demanderait de vérifier que la condition est vraie pour tout $t=1, 2, \dots, n$. Cette hypothèse est bien plus facile à vérifier que l'hypothèse TS.3 qui suppose que le terme d'erreur en $t$ n'est corrélé d'aucune manière que ce soit à toutes les variables explicatives pour toutes les périodes de temps. Ici, il faut simplement vérifier qu'il n'y ait pas de lien quelconque à la période $t$. Attention cela ne signifie pas que cette hypothèse supprime la possibilité pour que le terme d'erreur contienne des variables retardées. Dans ce cas il faut les prendre en compte dans le modèle.

**Sous les hypothèses TS.1', TS.2' et TS.3', les estimateurs MCO sont consistants :** $\text{plim }\hat{\beta} = \beta$**.**

-   **Hypothèse TS.4' : Homoscédasticité**

Les erreurs sont simultanément homoscédastiques : $Var(u_t|X_t) = \sigma^2$

Une foi encore, la stationnarité nous permet de ne vérifier que la relation contemporaine et pas pour toutes les valeurs de temps.

-   **Hypothèse TS.5' : Absence d'autocorrélation**

Pour tout $t \neq s$, $E(u_t u_s |X_t X_s) = 0$. Cette hypothèse signifie simplement que les termes d'erreurs de périodes de temps différentes ne doivent pas être corrélés.

**De manière asymptotique (lorsque le nombre d'observation croit vers l'infini), sous les hypothèses TS.1' à TS.5', les estimateurs MCO sont asymptotiquement normalement distribués. De plus les écarts-types, t-stat, F-stat et LM-stat issus des MCO sont asymptotiquement valides.**

Les modèles ayant des variables explicatives avec tendance peuvent satisfaire les hypothèses TS.1' à TS.5' à condition que ces variables soient stationnaires en tendance (processus Trend Stationnary). Aussi longtemps que les tendances sont intégrées dans l'équation si cela est nécessaire, la procédure d'inférence usuelle est asymptotiquement valide.

## Modèles à Retards Distribués (DL)

Les modèles à retards distribués sont des modèles dynamiques de séries temporelles dans lesquels la dynamique de la variable $Y$ peut être expliquée par des valeurs contemporaines et/ou retardées des variables explicatives $X$.

Ce type de modèle permet d'incorporer une dynamique plus riche et d'obtenir des effets marginaux (parfois causaux) des variables $X$ sur la variable $Y$.

Suivant si l'on considère un nombre fini ou infini de valeurs retardées pour la variable explicative, on parlera de modèles à retards distribués finis ou infinis. En pratique seuls les modèles à retards finis peuvent être estimés.

Un modèle à retards distribués finis d'ordre $DL(q_1, \dots, q_k)$ s'écrit sous sa forme générale :

$$
Y_t = \alpha + \sum_{i = 1}^k \sum_{j=0}^{q_j} \beta_{ij} X_{i, t-j} + \varepsilon_t
$$

Chaque variable $X$ peut avoir un nombre de retards qui lui est propre afin de capter au mieux la dynamique de la série $Y$.

Ces modèles souffrent souvent d'une forte multicolinéarité puisque les retards d'une variable $X_{it}$ sont souvent corrélés entre eux, ce qui réduit la précision des estimateurs.

Il est également courant que ce type de modèle n'arrive pas à capter toute la dynamique de la variable dépendante $Y_t$, à moins d'intégrer de nombreux retards pour chaque variable $X$. Les résidus restent souvent auto-corrélés rendant la variance des estimateurs non-minimale.

## Multiplicateurs et effets marginaux

### Dans un modèle statique

L'effet marginal d'une variable $X$ sur $Y$ est facilement déterminable dans le cadre d'un modèle statique (toutes les variables sont prises à la même date). Ainsi dans le modèle suivant :

$$ Y_t = \alpha + \beta X_t + \varepsilon_t $$

L'effet marginal de $X_t$ sur $Y_t$ (l'impact d'une augmentation d'une unité de $X$ sur $Y$) est donné par :

$$ \frac{\partial Y_t}{\partial X_t} = \beta $$

La réponse de la variable $Y_t$ à un changement d'une unité de la variavle $X_t$ est supposée être immédiate et complète à la fin de la période de mesure.

### Dans un modèle à retards distribués

Considérons le modèle à retards distribués d'ordre 2 suivant :

$$
Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \beta_2 X_{t-2} + u_t
$$

Pour interpréter, les coefficients de cette équation, supposons que $X$ soit constant et égal à $c$ (on se trouve sur une relation d'équilibre). On suppose une augmentation temporaire de $X$ d'une unité à la période $t$. On a donc :

$$
X_{t-2} = c \hspace{0.2cm}; X_{t-1} = c \hspace{0.2cm}; X_{t} = c +1 \hspace{0.2cm}; X_{t+1} = c \hspace{0.2cm}; X_{t+2} = c
$$

Afin de mettre l'accent sur l'effet toute chose égale par ailleurs, on considère que $u_t = 0$. On a :

$$
\begin{align}
Y_{t-1} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c \\
Y_t &= \alpha + \beta_0 \left( c+1 \right) + \beta_2 c + \beta_3 c \\
Y_{t+1} &= \alpha + \beta_0 c + \beta_1 \left( c+1 \right) + \beta_3 c \\
Y_{t+2} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 \left( c+1 \right) \\
Y_{t+3} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c
\end{align}
$$

En considérant les deux premières équations, on obtient : $Y_t - Y_{t-1} = \beta_0$ où $\beta_0$ représente le changement immédiat de $Y$ suite à une hausse de l'unité $X$ à la période $t$, ce que l'on peut noter $\frac{\partial Y_t}{\partial X_t}$ et est communément appelé le multiplicateur de court-terme.

De la même manière $Y_{t+1} - Y_{t-1} = \frac{\partial Y_{t+1}}{\partial X_t} = \beta_1$ et correspond à la variation de $Y$ une période après le changement temporaire de $X$.

$Y_{t+2} - Y_{t-1} = \frac{\partial Y_{t+2}}{\partial X_t} = \beta_2$ correspond à la variation de $Y$ une période après le changement temporaire de $X$. A la période $t+1$, $Y$ reprend sa valeur initiale car on a supposé que seuls deux retards de $X$ pouvaient avoir un impact sur $Y$. Ces multiplicateur sont aussi appelés *Delays multipliers*.

On peut également s'intéresser au changement de $Y$ à la suite d'une augmentation permanente de $X$. Considérons que $X$ augmente d'une unité de manière permanente à la période $t$ soit :

$$
X_{t-2} = c \hspace{0.2cm}; X_{t-1} = c \hspace{0.2cm}; X_{t} = c +1 \hspace{0.2cm}; X_{t+1} = c+1 \hspace{0.2cm}; X_{t+2} = c+1
$$

On a donc :

$$
\begin{align}Y_{t-1} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c \\
Y_t &= \alpha + \beta_0 \left( c+1 \right) + \beta_2 c + \beta_3 c \\
Y_{t+1} &= \alpha + \beta_0 \left( c+1 \right) + \beta_1 \left( c+1 \right) + \beta_3 c \\
Y_{t+2} &= \alpha + \beta_0 \left( c+1 \right) + \beta_1 \left( c+1 \right) + \beta_2 \left( c+1 \right)\end{align}
$$

A la suite de l'augmentation permanente de $X$, après une période $Y$ a augmenté de $\beta_0 + \beta_1$. Après deux périodes, $Y$ a augmenté de $\beta_0 + \beta_1 + \beta_2$. La somme des coefficients de $X$ et de ses deux retards représente la variation de long-terme de $XY$ suite à un changement permanent de $X$. On l'appelle le multiplicateur de long-terme. Les valeurs intermédiaires sont appelées *Interim multipliers*.

## Modèles Auto-Régressifs à Retards Distribués (ARDL)

Afin de rendre les modèles plus parcimonieux et essayer de capter la majeure partie de la dynamique de la variable dépendante, il est courant d'introduire une/des valeurs retardées de la variable $Y$ en plus des valeurs retardées des variables $X$. Cela permet de tenir plus facilement compte du caractère auto-corrélées de la majeure partie des séries temporelles étudiées en économie.

Sous sa forme générale, un $ARDL(p, q_1, \cdots, q_k)$ s'écrit :

$$
Y_t = \alpha + \sum_{l = 1}^p \phi_l Y_{t-l} + \sum_{i = 1}^k \sum_{j=0}^{q_i} \beta_{ij} X_{i, t-j} + \varepsilon_t
$$

De manière classique, $\varepsilon_t$ doit être un bruit blanc (non-auto-corrélé et de variance constante).

De la même manière que pour les modèles $ARMA$, il existe différentes façon (non-exclusives) de déterminer les retards $p$ et $q_i$ d'un modèle $ARDL$ :

-   Tester la significativité des paramètres $\phi_p$ et des différents $\beta_{i,q_i} \hspace{0.2cm} \forall \hspace{0.2cm} i = \{1, \dots, k\}$ .

-   Utiliser les critères d'informations (AIC, BIC,...) qui vont chercher la spécification qui permet le mieux de reproduire les données en ajoutant une contrainte de parcimonie.

-   Utiliser les tests d'autocorrélation des résidus (Ljung-Box) : augmenter l'ordre autorégressif jusqu'à ce que les résidus du modèle soient compatibles avec un bruit blanc.

Les modèles $ARDL$ peuvent être estimés à partir de la méthode des MCO.

## Cointégration

De manière générale, il n'est pas correct d'utiliser des variables non-stationnaires (caractérisées par un processus $I(1)$) dans des modèles de régression. En effet, lorsque de telles variables sont utilisées les estimateurs MCO ne sont généralement plus convergent. De plus lorsque deux variables $I(1)$ sont régressées l'une sur l'autre, il a été montré qu'une régression MCO indiquera souvent l'existence d'une relation statistique entre ces deux variables, même lorsqu'elles n'ont aucune raison d'être liées. Il s'agit du problème de *corrélation fallacieuse*.

Si $Y_t$ et $X_t$ sont des processus $I(1)$ aucunement liés entre eux, alors la régression suivante est fallacieuse et risque d'indiquer l'existence d'une relation qui n'existe pas :

$$
Y_t = \alpha + \beta X_t + u_t
$$

Généralement, pour contrer ce problème, on va stationnariser les variables en les prenant en première différence par exemple. Ainsi le modèle à estimer devient :

$$
\Delta Y_t = \gamma+ \delta\Delta X_t + e_t
$$

Avec $\Delta Y_t = Y_t - Y_{t-1}$. Mais cette régression n'est ps équivalente à la précente. $\gamma$ donne une information bien différente de $\beta$. La deuxième régression explique la différence de $Y$ en terme de différence de $X$ ce qui n'a rien à voir avec l'explication de $Y$ en fonction de $X$.

La notion de cointégration introduire par **Engle et Granger (1987)** permet dans certain cas de résoudre ce problème de régression fallacieuse.

En général, si $Y_t$ et $X_t$ sont des processus $I(1)$, alors $Y_t - \beta X_t$ sera un processus $I(1)$ pour n'importe quelle valeur de $\beta$.

Cependant, il est possible que pour une certaine valeur $\beta \neq 0$, alors $Y_t - \beta X_t$ soit un processus $I(0)$. Si un tel $\beta$ existe, alors on dira que $Y$ et $X$ sont cointégrés et $\beta$ sera appelé le paramètre de cointégration. Dans ce cas, les deux séries possèdent une relation de long-terme commune. Il peut y avoir des déviations par rapport à cet équilibre de long-terme, mais il existe des forces économiques qui vont ramener les deux variables vers leur relation d'équilibre. La différence entre les deux variables a tendance à revenir vers sa valeur moyenne. Dans ce cas, la régression entre $Y_t$ et $X_t$ peut se faire sans être fallacieuse.

En revanche, puisque $X_t$ est un processus $I(1)$, les procédures d'inférence usuelles ne s'appliquent pas nécessairement. De manière analogue aux modèles de régression linéaire, plusieurs hypothèses sont nécessaires à l'obtention d'une distribution normale du paramètre $\beta$ et d'une distribution exacte de la statistique de Student. La plupart peuvent se corriger en augmentant la taille de l'échantillon (normalité des résidus) ou en corrigeant les écarts-types obtenus (sphéricité des erreurs), ce n'est pas le cas pour l'hypothèse d'exogénéité stricte des régresseurs. Cette hypothèse implique dans le cas de séries temporelles fortement persistantes (dont font partis les processus $I(1)$) que les régresseurs $X$ ne soient liés d'aucune manière que ce soit avec les erreurs pour toutes les périodes de temps possibles. Il s'agit d'une hypothèse extrêmement forte qu'il est difficile de montrer sa validité en pratique.

Pour contrecarrer (en partie) ce problème, il est possible d'inclure $\Delta X_t$ ainsi que d'éventuels retards de cette variable dans l'équation de régression afin de purger la corrélation entre $X$ et $u$. Dans son cas le plus simple, la régression devient :

$$
Y_t = \alpha + \beta X_t + \Delta X_t + \varepsilon_t
$$

Le choix d'inclure ou non des retards additionnels est une question empirique qu'il convient de justifier.

## Modèles à correction d'erreur

Le concept de cointégration permet également d'enrichir les types de modèles dynamiques à notre disposition. Si $Y_t$ et $X_t$ sont des processus $I(1)$ non-cointégrés, alors la modélisation doit être une modélisation en différence première (généralement). Considérons le modèle ARDL suivant :

$$
\Delta Y_t = \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + u_t
$$

Dans le cas où $Y_t$ et $X_t$ sont cointégrés, nous disposons de variables supplémentaires que l'on peut inclure dans cette équation. En effet dans ce cas : $Y_t - \beta X_t$ est un processus $I(0)$, et ses retards peuvent sans problèmes être inclus dans une régression. L'équation devient donc :

$$
\begin{align}
\Delta Y_t &= \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta (Y_{t-1} - \beta X_{t-1}) + u_t \\
\Leftrightarrow \Delta Y_t &= \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta \text{ECT} + u_t
\end{align}
$$

Le terme $(Y_{t-1} - \beta X_{t-1})$ est appelé le terme à correcteur d'erreur (*Error Correction Term*), et ce type de modélisation est appelée Modèle à Correction d'Erreur.

Ce type de modèles permet d'étudier la dynamique de court terme dans la relation entre $Y$ et $X$. Si $\delta < 0$ et si $Y_{t-1} > \beta X_{t-1}$, alors $Y$ a dépassé l'équilibre durant la période précédente. Puisque $\delta < 0$, le terme à correction d'erreur fait en sorte que $Y$ est ramené vers l'équilibre en diminuant. A l'inverse, si $Y_{t-1} < \beta X_{t-1}$, alors $Y$ était inférieur à la valeur d'équilibre à la période précédente. Dans ce cas, le terme à correction d'erreur induit une variation positive de $Y$ vers l'équilibre.

Le temps que mettent les séries à revenir à leur équilibre est donné par $\frac{1}{\delta}$.

## Procédure d'Engle-Granger

La procédure d'Engle-Granger est une procédure en deux étapes pour estimer des modèles à correction d'erreur. Attention la procédure présentée ici n'est valide que pour deux séries toutes les deux caractérisées par un processus $I(1)$.

### 1ère étape : Tester l'existence de la cointégration

Dans un premier temps, si les séries $Y$ et $X$ sont des processus $I(1)$, alors il convient de régresser $Y$ sur $X$ :

$$
Y_t = \alpha + \beta X_t + e_t
$$

On récupère ensuite la série des résidus $\hat{e}_t$ et l'on teste la stationnarité de cette série. Généralement on utilise le test ADF avec un modèle sans constante mais avec ou sans tendance. On estime la relation :

$$
\Delta \hat{e}_t = \phi \hat{e}_{t-1} + \sum_{i=1}^p \phi_i \Delta \hat{e}_{t-i} + u_t
$$

avec $\hat{e}_t = \hat{Y}_t - \hat{\alpha} - \hat{\beta}X_t$. On teste ensuite :

$$
\left\{    
\begin{array}{ll}       
H_0 : \phi = 0 \text{ ; } \hat{e}_t \text{ non stationnaire}  \\       
H_1 : \phi < 0 \text{ ; } \hat{e}_t \text{ stationnaire}     
\end{array}
\right.
$$

Avec la statistique de test :

$$
t_{\hat{\phi}} = \frac{\hat{\phi}}{\hat{\sigma}_{\hat{\phi}}}
$$

Attention, les valeurs critiques pour tester la stationnarité de cette série sont différentes de celles utilisées classiquement.

| Type de test  |  1%   |  5%   |  10%  |
|:-------------:|:-----:|:-----:|:-----:|
| Sans tendance | -3.90 | -3.34 | -3.04 |
| Avec tendance | -4.32 | -3.78 | -3.50 |

La règle de décision est la suivante :

-   Si $t_{\hat{\phi}} < t$ : on rejette l'hypothèse nulle. Les résidus sont stationnaires. Les séries $Y_t$ et $X_t$ sont cointégrées.

-   Si $t_{\hat{\phi}} > t$ : on ne peut pas rejeter la l'hypothèse nulle. Les résidus ne sont pas stationnaires. Les séries $Y_t$ et $X_t$ ne sont pas cointégrées.

### 2ème étape : Estimer le modèle à correction d'erreur

Si $\hat{e}_t$ est bien un processus $I(0)$, alors les séries $Y_t$ et $X_t$ sont cointégrées et caractérisées par une relation de long terme. La régression de l'une l'autre n'est plus une régression fallacieuse. On peut donc estimer le modèle en niveau (en incluant $\Delta X_t$ et ses retards et/ou ses valeurs avancées ainsi que les retards de $Y_t$ si besoin) et obtenir les coefficients de long-terme de la série.

On peut ensuite estimer le modèle à correction d'erreur dans lequel le terme à correction d'erreur correspond à la série des résidus $\hat{e}_t$ :

$$
\Delta Y_t = \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta \hat{e}_{t-1} + u_t
$$

Ce modèle nous permet d'obtenir des effets de court-terme avec les coefficients des variables en différence, ainsi que la vitesse d'ajustement des séries suite à un écart à leur relation de long-terme.

## Extension de la cointégration

La cointégration à la Granger peut évidemment être généralisée avec un nombre de variables supérieures à 2. Mais cette méthode requiert tout de même que l'ensemble des variables soient intégrées du même ordre. [**Pesaran et al (2001)**](https://onlinelibrary.wiley.com/doi/full/10.1002/jae.616?casa_token=PNv1tRySyY0AAAAA%3A871E331d0NLVpRpeJ4TghehXlefBXEznk49q2vbrkuWUMKenTGGqaPc_rK95X3aWp34rPcw8leVRkog) ont développé une méthodologie permettant de tester l'existence d'une relation entre les niveaux de $K$ variables que celles-ci soient intégrées d'ordre 1 et/ou d'ordre 0.

La méthodologie repose sur l'estimation de 3 modèles :

### Modèle ARDL général

La forme générale d'un $ARDL(p, q_1, \dots, q_k)$ est donnée par :

$$
Y_t = c_0 + c_1t + \sum_{i=1}^pb_{y,i}Y_{t-i} + \sum_{j=1}^k \sum_{l=0}^{q_j}b_{j,l}X_{j,t-l} + \varepsilon_t
$$

Ce modèle régresse la variable $Y_t$ sur une constante $c_0$, une possible tendance linéaire $c_1$, ses $p$ valeurs passées ainsi que les valeurs contemporaines et passées des variables $X_t$. Le choix des ordres $p, q_1, \dots, q_j$ est une question empirique.

Les multiplicateurs de court-terme sont donnés par les coefficients associés aux valeurs contemporaines des variables $X_{j,t}$. ces multiplicateurs indiquent la réponse de $Y_t$ suite à une variation temporaire d'une unité de $X_{j,t}$.

$$
\text{SR}_{\text{ARDL}} = \frac{\partial Y_t}{\partial X_{j,t}} = b_{j,0} \hspace{0.3cm} j \in \left\{ 1, \cdots, k \right\}
$$

Les multiplicateurs $\text{Delay}_{x_{j,s}}$ indiquent la réponse de la variable $Y_{t+s}$ suite à une variation temporaire de la variable $X_{j,t}$. Il s'agit d'une réponse directe. Il s'agit du coefficient associé au $s^{\text{ième}}$ retard de la variable $X_{j}$ auquel viennent s'ajouter les effets directs sur les périodes antérieures pondérées par les effet auto-régressifs de la variable $Y$.

$$
\text{Delay}_{x_{j,s}} = \frac{\partial Y_{t+s}}{\partial X_{j,t}} = b_{j,s} + \sum_{i=1}^{min \left\{ p,s \right\}} b_{y,i} \frac{\partial Y_{t+\left( s-i \right)}}{\partial X_{j,t}} \hspace{0.6cm} ; \hspace{0.3cm} b_{j,s} = 0 \hspace{0.3cm} \forall \hspace{0.3cm} s>q
$$

Les multiplicateurs $\text{Interim}_{x_{j,s}}$ indiquent la réponse de la variable $Y_{t+s}$ suite à une variation permanente de la variable $X_{j,t}$. Il s'agit de la somme des multiplicateurs $\text{Delay}_{x_{j,i}}$ avec $i = \{1, \dots, s\}$.

$$
\text{Interim}_{x_{j,s}} = \sum_{i=0}^s \text{Delay}_{x_{j,s}}
$$

Les multiplicateurs de long-terme indiquent la réponse totale de la variable $Y$ suite à un choc permanent sur la variable $X_{j,t}$. Il s'agit de la somme des coefficients associés à la variable $X_t$ et à ses retards divisé par 1 moins la somme des coefficients auto-régressifs de la variable $Y$.

$$
\text{LR}_{\text{ARDL}} = \frac{\partial Y_{t+\infty}}{\partial X_{j,t}} = \theta_j = \frac{\sum_{l=0}^{q_j} b_{j,l}}{1 - \sum_{i=1}^p b_{y,i}} \hspace{0.6cm} ; \hspace{0.3cm} j  \in  \left\{ 1, \cdots, k \right\}
$$

Le vecteur de coefficients $\hat{\theta}_j$ permet de construire l'équation de long-terme associé aux niveaux des variables :

$$
\hat{Y}_t^{\text{LT}} = \hat{\alpha} + \sum_{j=1}^k \hat{\theta}_j X_{j,t}
$$

### Modèle Unrestricted Error Correction Model (UECM)

La formule d'un modèle UECM dérivé du modèle ARDL précédent est la suivante :

$$
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t
$$

Ce modèle met en relation la première différence de la variable $Y_t$ avec les retards de sa première différence ainsi que les retards de la première différence des variables $X_t$ comme un $ARDL$ classique avec des variables en première différence. Les coefficients associés à ces variables en différences premières représentes la relation de court-terme entre $Y$ et $X$. Cependant le modèle UECM va également inclure les niveaux des variable $Y$ et $X_j$ retardés d'une période afin d'inclure les relations de long-terme entre les variables.

A partir de ce modèle il est possible de calculer les multiplicateurs de court-terme et de long-terme. Ces multiplicateurs sont strictement égaux aux multiplicateurs calculés à partir du modèle $ARDL$ en niveaux.

Les multiplicateurs de court-terme sont donnés par les coefficients associés aux valeurs contemporaines des premières différences de $X_j$.

$$
\text{SR}_{\text{UECM}} = \text{SR}_{\text{ARDL}} = \frac{\partial Y_t}{\partial X_{j,t}} = \omega_j \hspace{0.6cm} j \in \left\{ 1, \cdots, k \right\}
$$

Les multiplicateurs de long-terme sont donnés par la division du coefficient associé à la variable $X_{j,t}$ en niveau avec le coefficient associé à $Y_t$ en niveau multiplié par moins 1.

$$
\text{LR}_{\text{UECM}} = \text{LR}_{\text{ARDL}} = \frac{\partial Y_{t+\infty}}{\partial X_{j,t}} = \theta_j = \frac{\pi_j}{-\pi_y} \hspace{0.6cm} j \in \left\{ 1, \cdots, k \right\}
$$

De la même manière que pour le modèle $ARDL$, le vecteur de coefficient de long-terme $\hat{\theta}_j$ peut être utilisé pour construire l'équation de long-terme des variables en niveau :

$$
\hat{Y}_t^{\text{LT}} = \hat{\alpha} + \sum_{j=1}^k \hat{\theta}_j X_{j,t}
$$

Ce modèle apporte une précision supplémentaire par rapport au modèle $ARDL$ puisqu'il indique également la vitesse de convergence des séries suite à un écart à la relation d'équilibre à travers le coefficient $\pi_y$. En effet, si $Y_{t-1}$ augmente de 1 tandis que toutes les variables restent inchangées, cela signifie que $Y_{t-1}$ s'est écarté de sa valeur de long-terme. Le coefficient $\pi_y$ indique la force de rappel qui va ramener $Y_t$ vers sa valeur de long-terme. $\frac{1}{\pi_y}$ indique le temps nécessaire pour $Y$ retourne à sa valeur de long-terme suite à un déséquilibre.

### Modèle Restricted Error Correction Model (RECM)

A partir du modèle $ARDL$ il est également possible de construire une modélisation $RECM$ de la forme suivante :

$$
\Delta Y_t = c_0 + c_1t + \sum_{i=1}^{p-1} \psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_{j-1}} \Delta X_{j, t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \pi_y \text{ECT}_t = \varepsilon_t
$$

Ce modèle relie la première différence de la variable $Y$ avec les premières différences (retardées) des variable $X_j$. La différence avec le modèle UECM réside dans le fait que les variables en niveau ont été retirées et inclues dans le terme $\text{ECT}_t$. cela permet de gagner des degrés de liberté puisqu'il y a moins de coefficients à estimer. On augmente donc la précision de nos estimateurs en diminuant leur variance estimée. Il permet également d'obtenir plus de flexibilité concernant l'inclusion de tendance ou de constante (voir plus loin).

Il faut remarquer que ce modèle permet d'obtenir les même coefficients de court-terme que les modèles $ARDL$ et $UECM$ à partir des coefficients $\omega_j$. La force de rappel donnée par le coefficient $\pi_j$ est également la même que dans le modèle $UECM$.

Lors de la construction de ce modèle, 5 cas sont possibles (et vont conditionner les tests statistiques) :

-   **Cas 1 : Sans constante ni tendance dans les équation de court- et long-terme**

Dans ce cas, on va contrainte qu'il n'y ait ni tendance ni constante à la fois dans l'équation en différence et dans le terme à correction d'erreur. On a donc :

$$
\begin{align}
&c_0 = c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{align}
$$

Le terme à correction d'erreur en $t$ est donné la différence entre la valeur de $Y_{t-1}$ et sa valeur de long-terme en $t-1$ (sans constante ni tendance) calculée à partir des multiplicateurs de long-terme $\theta_j$. Il s'agit de la valeur du déséquilibre de $Y$ avec sa valeur de long-terme.

-   **Cas 2 : Sans tendance ni constante dans l'équation de court-terme ; sans tendance mais avec constante dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il n'y ait ni constante ni tendance dans l'équation en différence, mais on va inclure une constante dans le terme à correction d'erreur. On a donc :

$$
\begin{align}
&c_0 = c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \mu + \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{align}
$$

-   **Cas 3 : Sans tendance mais avec constante dans l'équation de court-terme ; sans tendance et sans constante dans l'équation de long-terme**

Dans ce cas, on va contraire à ce qu'il y ait une constante mais pas de tendance dans l'équation en différence. On ne va inclure ni tendance ni constante dans le terme à correction d'erreur. On a donc :

$$
\begin{align}
&c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{align}
$$

-   **Cas 4 : Sans tendance mais avec constante dans l'équation de court-terme ; sans constante et avec tendance dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il y ait une constante mais pas de tendance dans l'équation en différence. On va inclure une tendance linéaire dans le terme à correction d'erreur mais pas de constante. On a donc :

$$
\begin{align}
&c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \delta(t-1) + \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{align}
$$

-   **Cas 5 : Avec tendance et constante dans l'équation de court-terme ; Sans tendance ni constante dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il y ait une constante et une tendance dans l'équation en différence et à ce qu'il n'y ait ni constante ni tendance dans le terme à correction d'erreur. On a donc :

$$
\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
$$ Il faut bien noter que tous ces cas ne sont pas tous utilisables pour une même modélisation. En effet, on ne peut pas imposer une contrainte (ou une absence de contrainte) sur des paramètres qui n'ont pas été inclus dans le modèle $ARDL$ ou $UECM$ sous-jacent. Par exemple, si le modèle $ARDL$ n'inclue pas de tendance linéaire, il n'est pas possible d'utiliser les cas où on impose une tendance linéaire dans l'équation de court- ou long-terme puisque cette tendance n'existe pas dans le modélisation initiale.

### Bound F test / Bound t test

Les tests initiaux de cointégration ne fonctionnent généralement que si toutes les variables sont intégrées du même ordre ce qui dans un certain nombre de cas pratiques n'est pas réaliste. **Pesaran et al (2001)** ont développé deux tests statistique afin de tester s'il existe une relation de long-terme entre les variables en niveau et ce même si toutes les variables ne sont intégrées du même ordre.

Les valeurs critiques de ces tests ne sont pas standards. **Pesaran et al (2001)** ont tabulés des valeurs critiques lorsque toutes les variables sont intégrées d'ordre 0 et lorsque toutes les variables sont intégrées d'ordre 1. Cela donne un intervalle permettant de déterminer si un ensemble de variables $I(0)$ et $I(1)$ possèdent une relation de long-terme ou non.

Le premier test est un test de Wald (sorte d'équivalent au test de Fisher) qui va tester les coefficients associés aux variables en niveau sont tous conjointement différents de 0.

Ainsi à partir du modèle :

$$
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t
$$

On va tester (en fonction du cas de modèle $RECM$ choisi) :

-   **Cas 1, 3, 5**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq 0     
\end{array}
\right.
$$

-   **Cas 2 :**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = c_0 = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq c_0 \neq 0     
\end{array}
\right.
$$

-   **Cas 4**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = c_1 = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq c_1 \neq 0     
\end{array}
\right.
$$

Sous l'hypothèse nulle, ne présentent pas de relation de long-terme en niveau. Elles ne sont pas cointégrées. On estime donc un $ARDL$ en première différence classique. Sous l'hypothèse alternative, il existe une relation de long-terme entre les variables en niveau. On peut donc estimer un modèle à correction d'erreur.

Une fois la statistique de test obtenue (non reportée ici) notée $F$, il faut la comparer aux valeurs critiques tabulés notées $F_{I(0)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(0)$ et $F_{I(1)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(1)$. Cela donne l'intervalle : $[F_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} F_{I(1)}]$. La règle de décision est la suivante :

-   Si $F < F_{I(0)}$ alors on ne peut pas rejeter $H_0$. Il semblerait qu'il n'existe pas de relation en niveau entre les variables peu importe que les variables soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées.

-   Si $F > F_{I(1)}$} alors on peut rejeter l'hypothèse nulle. Il semblerait qu'il existe une relation en niveau entre les variables peu importe quelles soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées

-   Si $F \in [F_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} F_{I(1)}]$ alors on ne peut pas tirer de conclusion à partir de ce test. Il faut regarder variable par variables leur ordre d'intégration avant de faire des tests d'inférence.

Le deuxième test proposé par **Pesaran et al (2001)** est un dérivé du test de Student qui va tester si la force de rappel de long-terme est significativement différente de 0 dans un modèle $UECM$.

Ainsi à partir du modèle :

$$ 
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t 
$$

On va tester :

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = 0  \\       
H_1 : \pi_y  \neq 0     
\end{array}
\right.
$$

Une fois la statistique de test obtenue (non reportée ici) notée $t$, il faut la comparer aux valeurs critiques tabulés notées $t_{I(0)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(0)$ et $t_{I(1)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(1)$. Cela donne l'intervalle : $[t_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} t_{I(1)}]$. La règle de décision est la suivante :

-   Si $t < t_{I(0)}$ alors on ne peut pas rejeter $H_0$. Il semblerait qu'il n'existe pas de force de rappel de long-terme. Ce qui semblerait indiquer l'absence de relation en niveau entre les variables peu importe que les variables soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées.

-   Si $t > t_{I(1)}$} alors on peut rejeter l'hypothèse nulle. Il semblerait qu'il existe une force de rappel de long-terme. Ce qui semblerait indiquer la présence de relation en niveau entre les variables peu importe quelles soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées

-   Si $t \in [t_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} t_{I(1)}]$ alors on ne peut pas tirer de conclusion à partir de ce test. Il faut regarder variable par variables leur ordre d'intégration avant de faire des tests d'inférence.

# Code

## Setup

Charger les librairies et tous les objets de setup. Comme rien n'est chargé pour l'instant il faut :

-   Utiliser la syntaxe `here::here()` pour dire que l'on utilise la fonction `here()` du package `here`

-   Indiquer le chemin en entier jusqu'au script `setup.R`

```{r}
#| message: false
#| warning: false
# Exécute le script setup.R pour charger tous les éléments importants
source(here::here("02-codes", "utils", "setup.R"))
```

Nous voulons modéliser la monnaie réelle (agrégat M2) en fonction du revenu réel, du taux d'intérêt des obligations étatiques et du taux d'intérêt des dépôts bancaires au Danemark de 1974:Q1 à 1987:Q3.

Commençons par charger les données contenues dans l'objet `ARDL::denmark` et transformons le en objet de classe `tibble` pour plus de facilités à manipuler les données. (Si certaines fonctions imposent l'utilisation d'un objet TS, alors on le retransformera en temps voulu).

```{r}
# Visualiser à quoi ressemble l'objet denmark
ARDL::denmark |> 
  head()
```

Il s'agit d'un objet de classe TS (Time Series) contenant 5 colonnes pour les différentes variables. Les dates sont indiquées en temps que noms des lignes. Il faut donc penser à les récupérer sinon nous n'aurons pas les dates disponibles lorsque l'on va transformer cet objet en tibble.

```{r}
df <-
  # Charger les données
  ARDL::denmark |> 
  # Transformer en une tibble en indiquant que le nom des lignes doit être mis dans la variable "year"
  as_tibble(rownames = "year") |> 
  # Afficher l'objet
  print()
```

```{r}
# Le fait d'être en quaterly complique un peu la mise en graphique
df |> 
  # Ajouter une variable qui indique le numéro de ligne : permet d'avoir une variable continue sur l'axe des x
  mutate(t = row_number()) |> 
  # Regrouper toutes les valeurs des variables d'intérêt dans une variable nommée "value" et le nom des variables associées dans une variable nimmée "variable"
  pivot_longer(
    cols = !c(year, t),
    names_to = "variable",
    values_to = "value"
  ) |> 
  print() |> 
  # Faire un graphique en ligne
  ggplot(aes(x = t, y = value)) +
  geom_line() +
  # Séparer les variables dans différents panels avec chaque axe qui leur est propre
  facet_wrap("variable", scales = "free")
```

```{r}
# Avec un objet TS
plot(ARDL::denmark)
```

Il semblerait que les variables ne soient pas stationnaires. On peut le vérifier avec le test ADF par exemple. On va effectuer le test ADF sur chaque série. Pour éviter d'avoir à le faire manuellement pour chaque série, on peut faire une boucle `for` qui va nous permettre de boucler une opération sur un certain nombre d'éléments.

On peut cependant être plus efficace en utilisant le package `purrr`. La fonction `map()` permet d'appliquer une fonction sur chaque élément d'un vecteur ou d'une liste donné. Ainsi `map(x, f)` va appliquer la fonction `f` à chaque élément contenu dans `x`. La fonction `map2(x, y, f)` permet de faire la même chose sauf qu'il y a deux vecteurs simultanés `x` et `y` qui vont rentrer dans la fonction `f`. De base ces fonctions renvoient une liste. La fonction `map2_dfr()` permet simplement de rendre un tibble au lieu d'une liste.

```{r}
# Appliquer le test ADF sur chaque série
# La fonction a besoin de 2 élements : la série et son nom
purrr::map2_dfr(
  list(df$LRM, df$LRY, df$IBO, df$IDE), # Série à tester
  c("LRM", "LRY", "IBO", "IDE"), # Nom de la série
  # Créer une fonction prenant deux paramètres : série et name
  # Cette fonction exécuter la fonction adf_test_auto
  \(serie, name) dobby::adf_test_auto(
    serie, name, 
    lags = 20, 
    message = FALSE, 
    return_res = TRUE
  )
)
```

Les tests ADF nous indiquent que les séries LRY et IBO ne semblent pas stationnaires. Leur inclusion dans un modèle utilisant les MCO peut résulter en une corrélation fallacieuse. Les coefficients associés ont de très fortes chances d'être biaisés fortement.

## Régression statique

```{r}
summary(lm(LRM ~ LRY + IBO + IDE, data = df))
```

A partir du modèle statique, on peut voir que les variables LRY et IBO sont très fortement significatives et que le R² ajusté est de 0,9218 ce qui est extrêmement élevé et un signe de corrélation fallacieuse/de coefficients biaisés à cause de la non stationnarité.

Un moyen simple de prendre en compte la non stationnarité consiste à différencier les variables non-stationnaires avant de les inclure dans le modèle de régression. Pour cela on peut soit créer des variables dans le tibble `df`, soit on peut utiliser la fonction `dynlm::dynlm()` qui permet d'appliquer des opérateurs de séries temporelles.

```{r}
# Différencier les variables LRY et IBO
df_diff <- 
  df |> 
  # Modifier le dataframe df
  mutate(
    # Effectuer la même opération pour les variable sélectionnées
    across(
      .cols = c(LRY, IBO), # Sélectionner les variables LRY et IBO
      .fns = \(variable) variable - lag(variable), # Leur appliquer une fonction qui va faire la différence première
      .names = "diff_{.col}" # Créer une nouvelle variable qui va prendre le nom diff_ suivi du nom originel de la variable
    )
  ) |> 
  # Afficher le résultat
  print()
```

```{r}
reg_statique <- 
  lm(LRM ~ diff_LRY + diff_IBO + IDE, data = df_diff) |> 
  summary() |> 
  print()
```

```{r}
# Utiliser des opérateurs de séries temporelles directement
dynlm::dynlm(
  LRM ~ d(LRY) + d(IBO) + IDE, # d() -> fait la différence première d'une série 
  data = df |> ts() # Obligé de transformer le dataframe en objet Time Series pour faire la différenciation
) |> 
  summary()
```

On peut voir (les deux méthodes donnant strictement le même résultat) que les variables en premières différences ne sont plus significatives tandis que la variable IDE devient significatives. L'augmentation des taux d'intérêts bancaire fait diminuer fortement l'offre totale de monnaie M2 puisqu'il y a moins de crédits en circulation dans l'économie.

```{r}
dobby::check_univariate_autocorr(reg_statique$residuals, "résidus statiques", test_type = "Box-Pierce", return_output = FALSE)
```

On peut voir que nos résidus sont fortement autocorrélés. cela n'est pas surprenant puisque nous n'avons inclue aucune dynamique dans notre régression.

## Régression avec variable dépendante retardée

Pour prendre en compte plus de dynamique, on peut rajouter des retards dans les variables expliques ou bien ajouter des retards de la variable dépendante. Pour ce faire on peut soit créer les variables dans un dataframe, soit on utilise la fonction `dynlm()`.

```{r}
reg_dep_retard <- 
  dynlm::dynlm(
  LRM ~ L(LRM) + d(LRY) + d(IBO) + IDE,
  data = df |> ts()
) |> 
  summary() |> 
  print()
```

les variables indiquées par `L(x)` sont les variables retardées (s'il y a plusieurs retards, un chiffre indique le numéro du retard : `L(x,1)` sera donc le premier retard de la variable $x$). Les variables indiquées par `d(x)` sont les variables en différence. S'il y a un chiffre après, cela indique le niveau de la différence : `d(x,2)` = $x_t - x_{t-2}$.

```{r}
dobby::check_univariate_autocorr(reg_dep_retard$residuals, "résidus dépendante retardée", test_type = "Box-Pierce", return_output = FALSE)
```

Outre le R² ajusté très élevé qui est suspect, on peut voir que l'inclusion de la variable dépendante retardée permet de prendre en compte d'avantage d'autocorrélation à l'ordre 1. En revanche on modélise très mal l'autocorrélation de long-terme. Il s'agit d'un indice indiquant que l'on n'a pas pris en compte assez de retards dans notre modélisation.

## Modélisation ARDL

La fonction `dynlm()` est très puissante mais sa syntaxe peut vite devenir encombrante lorsque les retards et les différences s'enchainent. Le package `ARDL` permet d'obtenir les mêmes résultats mais avec une syntaxe bien moins volumineuse. On peut utiliser la même syntaxe que pour la fonction `dynlm()` mais au lieu d'avoir à indiquer chaque retard pour chaque variable, on indique simplement le nombre de retards que l'on souhaite inclure dans la modélisation pour chaque variable via le paramètre `order`.

```{r}
# Simplement un retard pour la variable dépendante
ARDL::ardl(
  LRM ~ diff_LRY + diff_IBO + IDE, # variables à utiliser
  data = df_diff, # Dataframe ou objet TS
  order = c(1, 0, 0, 0) # Nombre de retards à inclure pour chaque variable
) |>
  summary()
```

```{r}
# Ajouter des retards aux variables explicatives
ARDL::ardl(
  LRM ~ diff_LRY + diff_IBO + IDE, # variables à utiliser
  data = df_diff, # Dataframe ou objet TS
  order = c(1, 2, 3, 1) # Nombre de retards à inclure
) |>
  summary()
```

## Procédure de Pesaran et al (2001)

Comme nous l'avons vu, notre jeu de données est caractérisé par des variables $I(0)$ stationnaires et des variables $I(1)$ non-stationnaires. Cette disparité empêche d'utiliser la procédure d'Engle et Granger pour tester la cointégration de nos variables. Actuellement, on ne sait pas s'il existe une relation entre les niveaux de nos variables et s'il est donc correcte de modéliser nos séries selon une telle relation ou selon un modèle à correction d'erreur. La meilleure chose que l'on pouvait faire était donc de différencier nos variables $I(1)$ afin d'obtenir des variables $I(0)$ et les inclure dans la modélisation.

Cependant, cela est limitant car ce qui nous intéresse est le niveau des taux d'intérêts et pas leur taux de croissance. Pour pallier ce problème, on peut utiliser la méthode de Pesaran et al (2001) afin de tester s'il existe une relation entre les niveaux de nos variables ou non.

La première étape consiste à trouver et estimer le meilleur ARDL entre les niveaux de nos variables. Pour cela, on peut tester différentes spécifications, utiliser les tests sur les résidus ou bien utiliser les critères d'information. La fonction `ARDL::auto.ardl()` permet de trouver le meilleur ARDL en fonction du critère d'information choisi.

```{r}
# Tester "tous" les modèles et les classer en fonction de leur AIC 
models <- 
  ARDL::auto_ardl(
    LRM ~ LRY + IBO + IDE, # Variables à utiliser
    data = df, # Source des variables
    max_order = 5, # Nombre de retards maximum autorisé pour les variables
    selection = "AIC" # critère d'information à utiliser
  )

# Afficher le classement des modèles
models$top_orders |> 
  print()
```

On peut voir que le modèle ARDL qui minimise le critère AIC (pondération entre l'explication du modèle et le nombre de paramètres à estimer) est le modèle $ARDL(3,1,3,2)$. Le modèle avec simplement un retard sur la variable retardée est le modèle numéro 16.

Ces modèles étant déjà estimés, on peut obtenir les résultats associés :

```{r}
# Enregistrer le meilleur modèle dans une variables
ardl_3132 <- 
  models$best_model

# Afficher le résumé des résultats
ardl_3132 |> 
  summary()
```

Il semble que la capacité explicative de ce modèle soit très élevée. Cependant à ce stade on ne sait pas encore s'il existe une relation entre les variables en niveau. Pour cela on va estimer le modèle $UECM$ à partir du meilleur $ARDL$ en niveau (c'est à dire qu'on va choisir le même nombre de retards pour chaque variable en différence).

```{r}
# Estimer le modèle UECM à partir du meilleur ARDL en niveau trouvé
uecm_3132 <- ARDL::uecm(ardl_3132)

# Afficher les résultats
uecm_3132 |> 
  summary()
```

Le modèle $UECM$ nous permet d'utiliser le Bound F test de Pesaran et al (2001) qui va tester l'hypothèse nulle de non significativité conjointe des coefficients des variables en niveau. Pour cela on utilise la fonction `ARDl::bound_f_test()` qui va prendre comme argument, entre autre, un ARDL ou UECM (le modèle ARDL sera automatiquement transformé en UECM) et le "cas" indiquant si la constante et/ou la tendance doivent participer à l'équation de court- ou long-terme. Le paramètre `alpha` permet d'indiquer le niveau de significativité des valeurs critiques. En cas d'absence, seule la p.value est renvoyée.

```{r}
# Bound F test dans lequel la constante participe dans l'équation de long-terme. 
# Valeurs critiques pour un niveau de significativité de 1%
res_f_test <- 
  ARDL::bounds_f_test(
    uecm_3132, # UECM
    case = 2, # Constante dans l'équation de long-terme. Pas de tendance
    alpha = 0.01 # Niveau de significativité de 1%
  )

# Afficher le résultat et les valeurs critiques
res_f_test$tab
```

Le Bound F test nous donne l'intervalle suivant : $[3.67 ; 4.61]$. La valeur de la statistique calculée est $5.12$. Cette statistique est supérieure à la valeur critique supérieure $I(1)$. On rejette donc l'hypothèse nulle de non-significativité conjointe des coefficients en niveau au sein de 1%. Il semblerait qu'il existe une relation de entre les niveaux des variables sélectionnées.

```{r}
# Ne pas indiquer les valeurs critiques. 
ARDL::bounds_f_test(
    uecm_3132, # UECM
    case = 2, # Constante dans l'équation de long-terme. Pas de tendance
  )
```
