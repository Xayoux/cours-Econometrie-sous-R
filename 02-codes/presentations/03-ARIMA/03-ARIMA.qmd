---
title: "03-ARIMA"
author: "Romain CAPLIEZ"
format:
  html:
    embed-resources: true
    code-overflow: wrap
    toc: true
    toc-location: left
    toc-depth: 5
    toc-expand: 2
    include-in-header:
      text: |
        <style>
          p {
            text-align: justify
          }
        </style>
  pdf:
    include-in-header:
      text: |
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines=true, breaksymbol=, commandchars=\\\{\}}
        \DefineVerbatimEnvironment{verbatim}{Verbatim}{breaklines=true, breaksymbol=,commandchars=\\\{\}}
editor: visual
---

```{r}
#| eval: false
#| include: false
# A UTILISER POUR RENDER EN HTML ET PDF
quarto::quarto_render(
    here::here(
        "02-codes", 
        "presentations",
        "03-ARIMA", 
        "03-ARIMA.qmd"
    ), 
    output_format = "all"
)
```

# Cours

Une série temporelle est une suite de variables aléatoires indexée par le temps. Un tel processus est aussi appelé processus stochastique. Lorsque l'on collecte un échantillon de données temporelles, on obtient un résultat possible, ou une réalisation, d'un processus stochastique. Cette réalisation est unique car on ne peut pas revenir en arrière et recommencer le processus à nouveau. Toutefois, si certaines conditions dans l'histoire avaient été différentes, on aurait généralement obtenu une réalisation différente.

Les séries temporelles sont souvent auto-corrélées dans le temps, entraînant ainsi le besoin d'ajuster les modélisations déjà vues. S'il est possible comme en coupe instantanée de faire des modèles statiques tels que :

$$
Y_t = \alpha + \beta_1 X_t + u_t
$$

où la variable endogène à l'instant $t$ dépend de la valeur de la variable exogène à l'instant $t$, la possible présence d'auto-corrélation nécessite d'ajuster les modèles pour prendre en compte les périodes passées.

Dans ce chapitre nous allons uniquement modéliser une série $Y_t$ par ses valeurs passées (aussi appelés *lags* ou *retards*) et pas des chocs aléatoires.

## Stationnarité

Les séries temporelles et financières sont très souvent caractérisées par un non-stationnarité en moyenne et/ou en variance. Une série est dite non stationnaire si ses propriétés statistiques changent au cours du temps. Plus formellement, on dira qu'une série est stationnaire au second ordre si :

$$
\begin{aligned}
E(X_t^2) &< \infty \\
E(X_t) &= m \hspace{0.3cm} \forall \hspace{0.3cm} t \\
E(X_t,X_{t+h}) &= \gamma_h \hspace{0.3cm} \forall \hspace{0.3cm} t, \hspace{0.3cm} \forall \hspace{0.3cm} h
\end{aligned}
$$

Une série stationnaire au second ordre doit avoir une variance finie, une moyenne constante au cours du temps et son auto-covariance doit dépendre uniquement de la différence des temps $h$.

La non-stationnarité a des conséquences fortes au niveau économétriques. Les méthodes habituelles ne sont plus valables puisque les estimateurs ne sont plus convergents, ils convergent vers une variable aléatoire et non plus leur vraie valeur. Egalement, les distribution asymptotiques ne sont plus standards.

De plus, si l'on régresse une série non stationnaire sur une autre série non-stationnaire, on obtient (sauf cointégration) une régression fallacieuse, c'est à dire que l'on va croire à un pouvoir explicatif de la variable exogène, alors qu'en réalité la corrélation est simplement due au temps qui passe. Voir le site [spurious-correlations](#0) pour des exemples de corrélations fallacieuses.

Si une série est non-stationnaire, il est nécessaire de procéder à des ajustements afin de la rendre stationnaire et retrouver les bonnes propriétés des estimateurs (ou alors il faut utiliser des méthodes économétriques plus complexes qui prennent directement en compte cette non-stationnarité). Les techniques de stationnarisation vont dépendre du type de processus que suit la série temporelle :

-   Processus **Trend Stationnary (TS)** : Dans un processus TS, les conséquences d'un choc sont transitoires, la série revient à sa valeur pré-choc (étant donné le passage du temps). La non-stationnarité est déterministe. Pour stationnariser ce type de processus, il faut retirer la tendance de la série.

-   Processus **Difference Stationnary (DS)** : Dans un processus DS, les conséquences d'un choc sont permanentes. La série ne revient jamais à sa valeur pré-choc. La non-stationnarité est stochastique. Pour stationnariser une telle série, on applique un filtre aux différences. Généralement on va utiliser la différence-première.

![](../images/processus-ts-ds.png)

De multiples tests existent pour détecter la stationnarité tels que le test augmenté de Dickey-Fuller (ADF) ou de Phillips-Perron.

## Test ADF

### Procédure ADF

Pour tester le caractère stationnaire d'une série, on applique des tests de racine unitaire. Il en existe de nombreux, mais le plus connu est probablement le test augmenté de Dickey-Fuller (1979, 1981). IL vise à tester l'hypothèse nulle de présence d'au moins une racine unitaire (non-stationnaire) contre l'hypothèse alternative de non-présence de racine unitaire (stationnarité).

La test ADF repose sur une procédure en plusieurs étapes. Il est important d'effectuer les étapes dans l'ordre et de manière rigoureuse.

On va d'abord chercher à déterminer le "bon" modèle à utiliser pour tester l'hypothèse nulle puis une fois que ce modèle sera trouvé, on testera la présence ou non de racine unitaire. **Il ne faut pas tester la présence de racine unitaire sur chaque modèle.**

Les modèles possibles sont :

-   Avec constante et tendance

-   Avec constante et sans tendance

-   Sans constante ni tendance

La première étape consiste à estimer le modèle avec constante et tendance (modèle 3) suivant :

$$
\Delta Y_t = \lambda + \delta t + \phi Y_{t-1} + \sum_{j=1}^p \phi_j \Delta Y_{t-j} + \varepsilon_t
$$

On régresse la variation de notre variable $\Delta Y_t$ sur le premier retard de notre variable en niveau $Y_{t-1}$ (paramètre d'intérêt pour tester la présence de racine unitaire), une constante $\lambda$ et une tendance temporelle linéaire $\delta t$. Dans la version augmenté du test (présentée ici) on rajoute les retards de la variables en différence afin d'essayer d'obtenir des résidus bruit-blanc.

L'ordre $p$ a son importance puisqu'il doit permettre de blanchir les résidus. Pour le choisir, on peut tester les résidus et choisir l'ordre $p$ qui va arriver à blanchir les résidus (tests d'autocorrélation, etc...). Ou bien (ce que l'on fait généralement), on va choisir l'ordre $p$ qui va minimiser un critère d'information (voir plus bas pour une définition).

Une fois l'ordre $p$ choisi et le modèle estimé, on va tester la significativité de la tendance temporelle.

On teste l'hypothèse nulle de nullité de la tendance temporelle contre l'hypothèse alternative qu'il existe une tendance temporelle :

$$
\begin{cases}
H_0 : \delta = 0 \\
H_1 : \delta \neq 0
\end{cases}
$$

Pour cela on va calculer la statistique t de Student du coefficient $\delta$ tel que :

$$
t_{\hat{\delta}} = \frac{\hat{\delta}}{\hat{\sigma}_{\hat{\delta}}}
$$

Puis on va comparer cette statistique de test avec les valeurs critiques tabulés par Dickey et Fuller. Attention, les valeurs critiques usuelles ne s'appliquent pas sur ce test.

Les valeurs critiques sont les suivantes :

| T        | 1%   | 5%   | 10%  |
|----------|------|------|------|
| 100      | 3.53 | 2.79 | 2.38 |
| 250      | 3.49 | 2.79 | 2.38 |
| 500      | 3.49 | 2.78 | 2.38 |
| $\infty$ | 3.46 | 2.78 | 2.38 |

La règle de décision est standard :

-   Si $|t_{\hat{\delta}}| \geq t_{\text{ADF}}$ : On rejette l'hypothèse nulle de non-significativité de la tendance linéaire. On garde ce modèle et on effectue le test de racine unitaire sur ce modèle.

-   $|t_{\hat{\delta}}| < t_{\text{ADF}}$ : On ne peut pas rejeter l'hypothèse de non-significativité de la tendance linéaire. Le modèle ne comporte pas de tendance linéaire. On rejette donc l'utilisation de ce modèle. Il faut estimer un modèle avec constante et sans tendance.

Dans le cas, où la tendance linéaire n'est pas significative, il faut estimer le modèle avec constante et sans tendance (modèle 2) suivant :

$$
\Delta Y_t = \gamma + \phi Y_{t-1} + \sum_{j=1}^p \phi_j \Delta Y_{t-j} + \varepsilon_t
$$ Encore une fois, il faut tester si ce modèle est le bon modèle à utiliser. Pour cela on va tester la significativité de la constante :

$$
\begin{cases}
H_0 : \gamma = 0 \\
H_1 : \gamma \neq 0
\end{cases}
$$ On calcule la statistique $t$ de Student du coefficient $\gamma$ tel que :

$$
t_{\hat{\gamma}} = \frac{\hat{\gamma}}{\hat{\sigma}_{\hat{\gamma}}}
$$

Puis on va comparer cette statistique de test avec les valeurs critiques tabulés par Dickey et Fuller. Attention, les valeurs critiques usuelles ne s'appliquent pas sur ce test.

Les valeurs critiques sont les suivantes :

| T        | 1%   | 5%   | 10%  |
|----------|------|------|------|
| 100      | 3.22 | 2.54 | 2.17 |
| 250      | 3.19 | 2.53 | 2.16 |
| 500      | 3.18 | 2.52 | 2.16 |
| $\infty$ | 3.18 | 2.52 | 2.16 |

La règle de décision est standard :

-   Si $|t_{\hat{\gamma}}| \geq t_{\text{ADF}}$ : On rejette l'hypothèse nulle de non-significativité de la constante. On garde ce modèle et on effectue le test de racine unitaire sur ce modèle.

-   $|t_{\hat{\gamma}}| < t_{\text{ADF}}$ : On ne peut pas rejeter l'hypothèse de non-significativité de la constante. Le modèle ne comporte pas de constante. On rejette donc l'utilisation de ce modèle. Il faut estimer un modèle sans constante ni sans tendance.

Dans le cas, où la constante n'est pas significative, il faut estimer le modèle sans constante ni tendance (modèle 1) suivant :

$$
\Delta Y_t = \phi Y_{t-1} + \sum_{j=1}^p \phi_j \Delta Y_{t-j} + \varepsilon_t
$$ Pour ce modèle il n'y a rien d'autre à tester que la présence ou non de racine unitaire. Le test de racine unitaire est le même peu importe le modèle choisi. Ce sont simplement les valeurs critiques qui vont changer.

On teste :

$$
\begin{cases}
H_0 : \phi = 0 \hspace{0.3cm} \text{Présence de racine unitaire} \\
H_1 : \phi < 0 \hspace{0.3cm} \text{Non présence de racine unitaire}
\end{cases}
$$ On calcule la statistique $t$ de Student du coefficient $\phi$ tel que :

$$
t_{\hat{\phi}} = \frac{\hat{\phi}}{\hat{\sigma}_{\hat{\phi}}}
$$ Les valeurs critiques pour chaque modèles sont disponibles dans la sous-section suivante.

Ce test étant un test unilatéral et les valeurs critiques étant négatives, la règle de décision est différente de la règle habituelle. **Attention, il ne faut pas prendre la valeur absolue de la statistique de test**.

-   Si $|t_{\hat{\phi}}| \leq t_{\text{ADF}}$ : On rejette l'hypothèse nulle de présence d'au moins une racine unitaire. La série ne comporte donc pas de racine unitaire. La série est donc stationnaire et peut être utilisée dans une modélisation standard.

-   $|t_{\hat{\phi}}| > t_{\text{ADF}}$ : On ne peut pas rejeter l'hypothèse nulle de la présence d'au moins une racine unitaire. La série comporte donc au moins une racine unitaire. La série n'est pas stationnaire. Elle ne peut pas être utilisée dans une modélisation standard. Elle doit d'abord être stationnarisée (généralement en la log-différenciant).

### Valeurs critiques de Dickey-Fuller

#### Significativité de la tendance (modèle 3)

| T        | 1%   | 5%   | 10%  |
|----------|------|------|------|
| 100      | 3.53 | 2.79 | 2.38 |
| 250      | 3.49 | 2.79 | 2.38 |
| 500      | 3.49 | 2.78 | 2.38 |
| $\infty$ | 3.46 | 2.78 | 2.38 |

#### Significativité de la constante (modèle 2)

| T        | 1%   | 5%   | 10%  |
|----------|------|------|------|
| 100      | 3.22 | 2.54 | 2.17 |
| 250      | 3.19 | 2.53 | 2.16 |
| 500      | 3.18 | 2.52 | 2.16 |
| $\infty$ | 3.18 | 2.52 | 2.16 |

#### Significativité du terme auto-régressif

**Modèle 1 (Sans constante ni tendance)**

| T        | 1%    | 5%    | 10%   |
|----------|-------|-------|-------|
| 100      | -2.60 | -1.95 | -1.61 |
| 250      | -2.58 | -1.95 | -1.62 |
| 500      | -2.58 | -1.95 | -1.62 |
| $\infty$ | -2.58 | -1.95 | -1.62 |

**Modèle 2 (Avec constante sans tendance)**

| T        | 1%    | 5%    | 10%   |
|----------|-------|-------|-------|
| 100      | -3.51 | -2.89 | -2.58 |
| 250      | -3.46 | -2.88 | -2.57 |
| 500      | -3.44 | -2.87 | -2.57 |
| $\infty$ | -3.43 | -2.86 | -2.57 |

**Modèle 3 (Avec constante et tendance)**

| T        | 1%    | 5%    | 10%   |
|----------|-------|-------|-------|
| 100      | -4.04 | -3.45 | -3.15 |
| 250      | -3.99 | -3.43 | -3.13 |
| 500      | -3.98 | -3.42 | -3.13 |
| $\infty$ | -3.96 | -3.41 | -3.12 |

## Auto-corrélation et auto-corrélation partielle

La fonction d'auto-corrélation (ACF) d'une série mesure la corrélation entre $X_t$ et $X_{x+k}$ où $k$ est le nombre de périodes dans le futur. Cette fonction mesure la corrélation entre deux points étant donné un intervalle de temps donné.

La fonction d'auto-corrélation partielle (PACF) mesure la corrélation entre $X_t$ et $X_{t+k}$ après que la corrélation entre les retards intermédiaires ait été supprimé. Autrement dit, elle mesure la corrélation directe entre deux observations temporelles.

## Processus bruit blanc (BB)

Une série temporelle $Y_t$ est appelée **bruit blanc** si :

$$
\begin{aligned}
Y_t &= \varepsilon_t \\
E(\varepsilon_t) &= 0 \hspace{0.3cm} \forall \hspace{0.3cm} t \\
V(\varepsilon_t) &= \sigma_\varepsilon^2 \hspace{0.3cm} \forall \hspace{0.3cm} t \\
Cov(\varepsilon_t, \varepsilon_{t'}) &= 0 \hspace{0.3cm} \forall \hspace{0.3cm} t \neq t'
\end{aligned}
$$

```{r echo=FALSE}
#| message: false
set.seed(1234)
arima.sim(n = 100, list(order=c(0,0,0))) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus Bruit Blanc"
  )
```

Un bruit blanc est donc un processus de moyenne nulle, de variance constante et non auto-corrélé. Un bruit ne suit pas forcément une loi normale.

## Marche Aléatoire

Une série est appelée marche aléatoire (Random Walk) si :

$$
Y_t =  Y_{t-1} + \varepsilon_t
$$

Avec $\varepsilon_t \sim (0, \sigma_\varepsilon^2)$. Il s'agit d'un processus non-stationnaire dans lequel la valeur en $t$ de la série est définie entièrement par sa valeur en $t-1$ et une variation aléatoire. Une marche aléatoire est par définition imprévisible.

```{r echo=FALSE}
#| message: false
set.seed(124)
arima.sim(n = 100, list(order=c(0,1,0))) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus de marche aléatoire"
  )
```

## Processus Auto-Régressif (AR)

On appelle processus auto-régressif d'ordre $p$ noté $AR(p)$, un processus $Y_t$ stationnaire tel que :

$$
Y_t - \phi_1Y_{t-1} - \cdots - \phi_p Y_{t-p} = \varepsilon_t
$$

Avec $\phi_i \in R, i = 1, \cdots, p$ et $\varepsilon_t \sim BB(0, \sigma{\varepsilon}^2)$.

En introduisant l'opérateur retard, on peut écrire :

$$
\left( 1 - \phi_1 L - \phi_p L^p \right) Y_t = \varepsilon_t
$$

Ou encore sous forme compacte :

$$
\Phi \left( L \right) Y_t = \varepsilon_t
$$

Pour un $AR(p)$, les auto-corrélations partielles s'annulent à partir du rang $p+1$. Cette propriété peut être utilisée pour définir graphiquement l'ordre d'un processus $AR(p)$. La fonction d'auto-corrélation n'a pas de forme définie.

```{r, echo=FALSE, message=FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(3,0,0), ar = c(0.3, 0.3, 0.3))) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus AR(3) simulé"
  )
```

```{r, echo = FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(3,0,0), ar = c(0.3, 0.3, 0.3))) |> 
  acf(main = "ACF d'un AR(3) simulé") 
```

```{r, echo=FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(3,0,0), ar = c(0.3, 0.3, 0.3))) |> 
  pacf(main = "PACF d'un AR(3) simulé") 
```

## Processus Moyenne Mobile (MA)

On appelle processus $MA(q)$ un processus $Y_t$ stationnaire tel que :

$$
Y_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \cdots - \theta_q \varepsilon_{t-q}
$$

Avec : $\theta_i \in R, i=1, \cdots, q$ et $\varepsilon_t \sim BB \left(0, \sigma_{\varepsilon}^2 \right)$

Avec l'opérateur retard, on a :

$$
Y_t = \left( 1 - \theta_1 L - \cdots - \theta_q L^q \right)\varepsilon_t
$$

ou encore :

$$
Y_t = \Theta \left( L \right)\varepsilon_t
$$

Pour un processus $MA(q)$, la fonction d'auto-corrélation s'annule à partir du rang $q+1$. La fonction d'auto-corrélation partielle n'a pas de forme définie.

```{r, echo=FALSE, message=FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(0,0,3), ma = c(0.3, 0.3, 0.3))) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus MA(3) simulé"
  )
```

```{r, echo=FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(0,0,3), ma = c(0.3, 0.3, 0.3))) |> 
  acf(main = "ACF d'un MA(3) simulé") 
```

```{r, echo = FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(0,0,3), ma = c(0.3, 0.3, 0.3))) |> 
  pacf(main = "ACF d'un MA(3) simulé") 
```

## Inversion des processus

Si le polynome $\Theta (L)$ est inversible, on peut écrire un processus $MA(q)$ sous la forme d'un $AR(\infty)$ :

$$ \varepsilon_t = \Theta^{-1} \left( L \right)Y_t = \sum_{i=0}^{\infty} \beta_i Y_{t-i} $$

Avec $\beta_0 = 1$ et $\sum \left| \beta_i \right| < \infty$.

Si le polynome $\Phi(L)$ est inversible, on peut écrire un processus $AR(p)$ sous forme $MA(\infty)$ :

$$ X_t = \Phi^{-1} \left( L \right) \varepsilon_t = \sum_{i=0}^{\infty} \alpha_i\varepsilon_{t-i} $$

Avec $\alpha_0 = 1$ et $\sum \left| \alpha_i \right| < \infty$.

## Processus ARMA(p,q)

Les processus $ARMA(p,q)$ permettent d'avoir une structure plus parcimonieuse des retards par rapport à un processus $AR(p)$ ou $MA(q)$.

Un processus $Y_t$ stationnaire suit un processus $ARMA(p,q)$ si :

$$
Y_t - \phi_1 Y_{t-1} - \cdots - \phi_p Y_{t-p} = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \cdots - \phi_q \varepsilon_{t-q}
$$

Avec : $\phi_i \in R, i=1, \cdots, q$ , $\theta_i \in R, i=1, \cdots, q$ et $\varepsilon_t \sim BB \left(0, \sigma_{\varepsilon}^2 \right)$

En introduisant l'opérateur retard, on a :

$$
\left( 1 - \phi_1L - \cdots - \phi_p L^{p} \right)Y_t = \left( 1 - \theta_1 L - \cdots - \theta_qL^q \right)\varepsilon_t
$$

Ou encore :

$$
\Phi \left( L \right) Y_t = \Theta \left( L \right)\varepsilon_t
$$

On peut déterminer graphiquement l'ordre d'un processus $ARMA(p,q)$ en regardant les ACF et PACF et en utilisant les mêmes règles que pour les processus $AR$ et $MA$.

```{r, echo = FALSE, message=FALSE}
set.seed(124)
arima.sim(n = 100, list(order=c(1,0,1), ar = 0.7, ma = 0.3)) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus ARMA(1,1) simulé"
  )
```

## Processus SARMA(p,q)(P,Q)

Lorsqu'une série présente de la saisonnalité, 2 grandes possibilités existent pour la prendre en compte dans notre modélisation :

-   corriger les séries des variations saisonnières

-   Modélisation la saisonnalité au moyen d'un processus SARMA

Un processus $SARMA(p,q)(P,Q)$ s'écrit :

$$
\begin{aligned}
&\left( 1 - a\phi_1L - \cdots - \phi_pL^p \right) \left( 1 - \phi_{1S}L_S - \cdots - \phi_{PS}L^P_S \right)Y_t \\ 
&= \left( 1 - \theta_1 L - \cdots - \theta_qL^q \right) \left( 1 - \theta_{1S} L_S - \cdots - \theta_{QS}L^Q_S \right)\varepsilon_t
\end{aligned}
$$

S'il existe différent type de saisonnalité, on utilise un polynôme saisonnier par saisonnalité.

## Processus ARIMA(p,d,q)

Si une série $Y_t$ est non stationnaire, cette non-stationnarité peut être estimée au moyen d'un processus $ARMA(p,q)$ intégré noté $ARIMA(p,d,q)$ ou $d$ est un nombre entier et est le paramètre d'intégration. Il s'agit du nombre de fois qu'il faut différencier une série pour la rendre stationnaire.

Un processus $ARIMA(p,d,q)$ s'écrit :

$$
\Phi \left( L \right) \left( 1-L \right)^d Y_t = \Theta \left( L \right)\varepsilon_t
$$

Estimer un processus $ARIMA(p,d,q)$ sur $Y_t$ revient à estimer un $ARMA(p,q)$ sur $\Delta^d Y_t$.

Rappel que : $\left( 1 - L^2 \right)Y_t =  \left( 1 - 2L + L^2 \right)Y_t$

```{r, echo = FALSE, message=FALSE}
set.seed(1238)
arima.sim(n = 100, list(order=c(1,1,1), ar = 0.7, ma = 0.3)) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(t = dplyr::row_number()) |> 
  ggplot2::ggplot(ggplot2::aes(x = t, y = x)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::labs(
    y = "Processus ARIMA(1,1, 1) simulé"
  )
```

## Processus ARFIMA(p,d,q)

Un processus $ARFIMA(p,d,q)$ est un procesus fractionnairement intégré. Il s'écrit :

$$ 
\Phi \left( L \right) \left( 1-L \right)^d Y_t = \Theta \left( L \right)\varepsilon_t 
$$

Avec $d$ un nombre réel fractionnaire. Un tel processus permet de modéliser les séries très persistantes à mémoire longue.

## Identification d'un processus ARMA(p,q)

Avant de modéliser notre série et d'estimer le processus choisi, il faut d'abord identifier le processus que suit la série.

### Déterminer l'ordre MA(q)

Pour déterminer l'ordre $q$ d'un processus $ARMA(p,q)$, on commence par calculer la fonction d'auto-corrélation de la série $Y_t$ :

$$
\hat{\rho}_k = \frac{\frac{1}{T} \sum_{t=1}^{T-k} \left( Y_t - \bar{Y} \right) \left( Y_{t+k} - \bar{Y} \right)}{\frac{1}{T} \sum_{t=1}^T \left( Y_t - \bar{Y} \right)^2}
$$

Avec $k = 1, 2, \cdots, K$. Box et Jenkins recommandent de choisir $K = \frac{T}{4}$.

$$
\begin{cases}
H_0 : \hat{\rho}_k = 0 \\
H_1 : \hat{\rho}_k \neq 0
\end{cases}
$$

avec sous $H_0$ : $\hat{\rho}_k \sim N(0, \sigma_\rho^2)$.

La règle de décision est classique :

-   Si $|t_{\hat{\rho}_k}| \geq 1.96$ (ou $p.value \leq 0.05$), on rejette l'hypothèse nulle. L'auto-corrélation d'ordre $k$ est significativement différente de 0.

-   Si $|t_{\hat{\rho}_k}| < 1.96$ (ou $p.value > 0.05$), on ne peut pas rejeter l'hypothèse nulle. L'auto-corrélation d'ordre $k$ n'est pas significativement différente de 0.

L'ordre du processus $MA$ correspond au dernier ordre à partir duquel on peut encore rejeter l'hypothèse nulle.

### Déterminer l'ordre AR(p)

Pour déterminer l'ordre $p$ d'un processus $ARMA(p,q)$, on calcule la fonction d'auto-corrélation partielle avec l'aide de l'algorithme de Durbin.

On teste :

$$
\begin{cases}
H_0 : \phi_{kk} = 0 \\
H_1 : \phi_{kk} \neq 0
\end{cases}
$$

Avec sous $H_0$ : $\phi_{kk} \sim N(0, \sigma_{\phi_{kk}}^2)$.

La règle de décision est classique :

-   Si $|t_{\hat{\phi}_{kk}}| \geq 1.96$ (ou $p.value \leq 0.05$), on rejette l'hypothèse nulle. L'auto-corrélation partielle d'ordre $k$ est significativement différente de 0.

-   Si $|t_{\hat{\phi}_{kk}}| < 1.96$ (ou $p.value > 0.05$), on ne peut pas rejeter l'hypothèse nulle. L'auto-corrélation partielle d'ordre $k$ n'est pas significativement différente de 0.

L'ordre du processus $AR$ correspond au dernier ordre à partir duquel on peut encore rejeter l'hypothèse nulle.

### Modèles identifiés

Les deux étapes précédentes nous ont permis d'identifier un ou plusieurs modèles possibles. Si un ordre $p = 3$ a été trouvé, cela signifie que les modèles $AR(p)$ possibles sont : $AR(1)$, $AR(2)$ et $AR(3)$. Si un ordre $q=2$ a été trouvé, cela signifie que les modèles $MA(q)$ possibles sont : $MA(1)$ et $MA(2)$.

Les processus $ARMA(p,q)$ possibles correspondent à toutes les combinaisons possibles de ces différents processus.

## Estimation des processus ARMA

Pour estimer un processus $ARMA(p,q)$, on utilise la méthode du maximum de vraisemblance. Cette méthode consiste à maximiser la log-vraisemblance d'un modèle afin que les valeurs prédites soient le plus proche possible des vraies valeurs de la série. On suppose généralement que $\varepsilon_t \sim N(0, \sigma_\varepsilon^2)$.

La log-vraisemblance d'un modèle $ARMA(p,q)$ s'écrit :

$$
log(L_t) = - \frac{T}{2} log(2 \pi) - \frac{T}{2} log(\sigma_{\varepsilon}^2) - \frac{T}{2} log \left( \text{det}(ZZ' ) \right) - \frac{S(\phi, \theta)}{2 \sigma_{\varepsilon}^2} 
$$

Avec $Z$ une matrice dépendant des paramètres $\phi$ et $\theta$ et

$$
S(\phi, \theta) = \sum \left( E \left[ \varepsilon_t | X_t, \phi_i, \theta_j, \sigma_{\varepsilon}^2 \right] \right)^2
$$

On cherche à maximiser $Log(L_t)$ par rapport à : $\phi_i, \theta_j, \sigma_{\varepsilon}^2$.

Le but de cette étape est d'estimer les paramètres $\phi_i$ et $\theta_j$ pour l'ensemble des modèles identifiés.

## Validation des processus ARMA

Cette étape consiste à valider les modèles estimés, puis à les départager. Pour cela, on va appliquer un certain nombre de tests sur les résidus afin de déterminer s'ils suivent bien un bruit blanc. Sinon cela veut dire que des caractéristiques de la série n'ont pas été prises en compte dans la modélisation.

On va également appliquer des tests de significativité sur les paramètres afin de garder les modèles avec des paramètres significatifs.

Enfin, on va départager les modèles ayant passé les différents tests en utilisant les critères de sélection.

### Tests sur les paramètres

Le but ici est de tester si les derniers retards inclus sont significativement différents de 0 ou non. Si ce n'est pas le cas, leur inclusion n'est guère utile et on va privilégié un modèle plus parcimonieux.

On peut donc tester $H_0 : p' = p-1, q' = q$

$$
\begin{cases}
H_0 : \text{ARMA}(p-1, q) &\Leftrightarrow \phi_p = 0 \\
H_1 : \text{ARMA}(p,q)& \Leftrightarrow \phi_p \neq 0
\end{cases}
$$

La règle de décision est classique :

-   Si $|t_{\hat{\phi}_p}| \geq 1.96$ (ou $p.value \leq 0.05$), on rejette l'hypothèse nulle de non significativité du coefficient. On valide donc l'inclusion du $p^{\text{ième}}$ coefficient.

-   Si $|t_{\hat{\phi}_p}| < 1.96$ (ou $p.value >0.05$), on ne peut pas rejeter l'hypothèse nulle de non significativité du coefficient. On ne valide donc pas l'inclusion du $p^{\text{ième}}$ coefficient. Il faut donc estimer puis tester un $ARMA(p-1,q)$.

On effectue le même test sur le coefficient $q$ :

On peut donc tester $H_0 : p' = p, q' = q-1$

$$ \begin{cases} H_0 : \text{ARMA}(p, q-1) &\Leftrightarrow \theta_q = 0 \\ H_1 : \text{ARMA}(p,q)& \Leftrightarrow \theta_q \neq 0 \end{cases} $$

La règle de décision est classique :

-   Si $|t_{\hat{\theta}_q}| \geq 1.96$ (ou $p.value \leq 0.05$), on rejette l'hypothèse nulle de non significativité du coefficient. On valide donc l'inclusion du $q^{\text{ième}}$ coefficient.

-   Si $|t_{\hat{\theta}_q}| < 1.96$ (ou $p.value >0.05$), on ne peut pas rejeter l'hypothèse nulle de non significativité du coefficient. On ne valide donc pas l'inclusion du $q^{\text{ième}}$ coefficient. Il faut donc estimer puis tester un $ARMA(p,q-1)$.

Le but ici est d'éliminer l'ensemble des modèles dont les derniers retards ne sont pas significatifs.

### Tests sur les résidus

#### Absence d'auto-corrélation

Les tests d'auto-corrélation servent à déterminer si toute la dynamique de la série a bien été prise en compte ou pas. Les résidus ne doivent pas être auto-corrélé, sinon cela signifie qu'une partie de la dynamique a été mise de coté.

Les tests les plus utilisés sont les tests de Box-Pierce et de Ljung-Box

##### Test de Box-Pierce

On teste l'hypothèse nulle d'absence d'auto-corrélation dans les résidus.

$$
\begin{cases}
H_0 : \hat{\rho}_{\hat{\varepsilon}1} = \cdots = \hat{\rho}_{\hat{\varepsilon}K} \\
H_1 : \exists \hspace{0.2cm} \hat{\rho}_{\hat{\varepsilon}k} \neq 0
\end{cases} 
$$

Une fois le modèle estimé, on récupère donc la série des résidus et on calcule la statistique de test :

$$
BP(K) = T \sum_{k=1}^K \hat{\rho}_{\hat{\varepsilon}k} \sim \chi^2(K-p-q)
$$

Avec $\hat{\rho}_{\hat{\varepsilon}k}$ le coefficient d'auto-corrélation des résidus à l'ordre $k$.

La règle de décision est la suivante :

-   Si $BP(K) \geq \chi^2(K-p-q)$ (ou si $p.value \leq 0.05$), on rejette $H_0$. Les résidus sont auto-corrélés à l'ordre $k$. Le modèle n'est pas validé. Toute la dynamique de la série n'est pas identifiée.

-   Si $BP(K) < \chi^2(K-p-q)$ (ou si $p.value > 0.05$), on ne peut pas rejeter $H_0$. Les résidus ne sont pas auto-corrélés à l'ordre $k$. Le modèle est validé de ce point de vue. Toute la dynamique de la série semble identifiée.

##### Test de Ljung-Box

Ce test est à privilégier dans le cadre de petits échantillons.

On teste l'hypothèse nulle d'absence d'auto-corrélation dans les résidus :

$$
\begin{cases}
H_0 : \hat{\rho}_{\hat{\varepsilon}1} = \cdots = \hat{\rho}_{\hat{\varepsilon}K} \\
H_1 : \exists \hspace{0.2cm} \hat{\rho}_{\hat{\varepsilon}k} \neq 0
\end{cases} 
$$

Une fois le modèle estimé, on récupère donc la série des résidus et on calcule la statistique de test :

$$
LB(K) = T(T+2) \sum_{k=1}^K \frac{\hat{\rho}_{\hat{\varepsilon}k}^2}{T-K} \sim \chi^2(K-p-q)
$$

Avec $\hat{\rho}_{\hat{\varepsilon}k}$ le coefficient d'auto-corrélation des résidus à l'ordre $k$.

La règle de décision est la suivante :

-   Si $LB(K) \geq \chi^2(K-p-q)$ (ou si $p.value \leq 0.05$), on rejette $H_0$. Les résidus sont auto-corrélés à l'ordre $k$. Le modèle n'est pas validé. Toute la dynamique de la série n'est pas identifiée.

-   Si $LB(K) < \chi^2(K-p-q)$ (ou si $p.value > 0.05$), on ne peut pas rejeter $H_0$. Les résidus ne sont pas auto-corrélés à l'ordre $k$. Le modèle est validé de ce point de vue. Toute la dynamique de la série semble identifiée.

#### Homoscédasticité

La variance des résidus doit être constante. Sinon cela signifie que des éléments de la dynamique n'ont pas été pris en compte.

##### Test de White

On récupère la série des résidus $\hat{\varepsilon}_t$ du modèle.

On estime une régression du type :

$$
\hat{\varepsilon}_t^2 = a_0 + a_1 Y_{t-1} + b_1 Y_{t-1}^2 + \cdots + a_p Y_{t-p} + b_p Y_{t-p}^2 + u_t
$$

On teste l'hypothèse nulle de variance constante (homoscédasticité) :

$$
\begin{cases}  H_0 : a_i = b_i = 0 \hspace{0.3cm} \forall i = 1, \cdots, p \\  H_1 : \exists \hspace{0.2cm} a_i \neq 0 \hspace{0.2cm} | \hspace{0.2cm} b_i \neq 0\end{cases}
$$

On calcule la statistique de test :

$$
LM = TR^2 \sim \chi^2(2p)
$$

Avec $R^2$ le coefficient de détermination de la régression auxiliaire estimée.

La règle de décision est la suivante :

-   Si $LM \geq \chi^2(2p)$ (ou $p.value \leq 0.05$), on rejette $H_0$. Les résidus n'ont pas une variance constante et sont donc hétéroscédastiques. Toute la dynamique de la série n'a pas été prise en compte. Le modèle est invalidé.

-   Si $LM < \chi^2(2p)$ (ou $p.value >0.05$), on ne peut pas rejeter $H_0$. Les résidus ont une variance constante et sont donc homoscédastiques. Toute la dynamique de la série semble prise en compte. Le modèle est validé de ce point de vue.

##### Test ARCH

Le test ARCH vise à déterminer si la série suit un processus Autoregressif Conditionnellement Hétéroscédastique (ARCH). Un tel processus est caractérisé par une variance non-constante au cours du temps et dont la dynamique (de la variance) dépend de ses propres valeurs passées. Cela amène à des phénomènes de *cluster* de volatilité très courants avec des séries financières.

On estime le modèle, puis l'on récupère la série des résidus $\hat{\varepsilon}_t$.

On estime la régression suivante :

$$
\hat{\varepsilon}_t^2 = \alpha_0 + \sum_{i=1}^l \alpha_i \hat{\varepsilon}_{t-i}^2
$$

On calcule ensuite le coefficient de détermination $R^2$ de cette régression.

On teste l'hypothèse nulle d'absence d'hétéroscédasticité conditionnelle :

$$
\begin{cases}  
H_0 : \alpha_i = 0 \hspace{0.3cm} \forall \hspace{0.2cm} i = 1, \cdots, l \\  
H_1 : \exists \hspace{0.2cm} \alpha_i \neq 0
\end{cases}
$$

On calcule la statistique :

$$
LM = TR^2 \sim \chi^2(l)
$$

La règle de décision est la suivante :

-   Si $LM \geq \chi^2(l)$ (ou $p.value \leq 0.05$), on rejette $H_0$. Les résidus n'ont pas une variance conditionnelle constante et sont donc conditionnellement hétéroscédastiques. Toute la dynamique de la série n'a pas été prise en compte. Le modèle est invalidé.

-   Si $LM < \chi^2(2p)$ (ou $p.value >0.05$), on ne peut pas rejeter $H_0$. Les résidus ont une variance conditionnelle constante et sont donc conditionnellement homoscédastiques. Toute la dynamique de la série semble prise en compte. Le modèle est validé de ce point de vue.

## Sélection d'un processus ARMA

A l'issu des différents tests, il est possibles que différents modèles soient sélectionnés. Pour choisir le modèle à garder, on va utiliser des critères de choix de modèles.

### Critères standards

Ces critères sont basés sur l'erreur de prévision des valeurs valeurs prédites $\hat{\varepsilon}_t$ que l'on va chercher à minimiser. On peut utiliser les critères suivants :

-   Erreur Absolue Moyenne (MAE)

$$
\text{MAE} = \frac{1}{T} \sum_t \left| \hat{\varepsilon}_t \right|
$$

-   Racine de l'Erreur Quadratique Moyenne (RMSE)

$$
\text{RMSE} = \sqrt{\frac{1}{T} \sum_t \hat{\varepsilon}_t^2}
$$

-   Écart Absolu Moyen en % (MAPE)

$$
\text{MAPE} = 100 \times \frac{1}{T} \sum_t \left| \frac{\hat{\varepsilon}_t}{Y_t} \right|
$$

### Les critères d'information

Les critères d'information opèrent un arbitrage entre la bonne qualité d'ajustement du modèle et la parcimonie du modèle. Ces critères sont à minimiser et vont indiquer le modèle qui sera le plus parcimonieux possible tout en gardant une bonne qualité de prédiction.

-   Akaike (AIC)

$$
\text{AIC} = log \left( \hat{\sigma}_{\varepsilon}^2 \right) + \frac{2(p+q)}{T}
$$

-   Schwarz (SIC)

$$
\text{SIC} = log \left( \hat{\sigma}_{\varepsilon}^2 \right) + (p+q) \frac{log(T)}{T}
$$

-   Hanan-Quinn (HQ)

$$
\text{HQ} = log \left( \hat{\sigma}_{\varepsilon}^2 \right) + \alpha (p+q) log \left( \frac{log(T)}{T} \right)
$$

Avec $\alpha$ une constante que l'on choisit avec généralement $\alpha = 1$.

Généralement, le critère SIC aura tendance à retenir des modèles plus parcimonieux que le critère AIC. AIC est souvent le critère le plus utilisé.

On va chercher le modèle qui va minimiser le plus de critères d'informations possibles.

## Prévisions d'un processus ARMA

Soit un processus $ARMA(p,q) : \Phi(L) Y_t = \Theta(L) \varepsilon_t$ et soit $\hat{Y}_{t+k}$ la prévision faite en $t$ pour la date $t+k$ où $k$ est l'horizon de prévision.

La prévision est donnée par :

$$
\hat{Y}_{t+k} = E \left[ Y_{t+k} \hspace{0.15cm} | \hspace{0.15cm} I_t  \right]
$$ Où $I_t$ correspond à l'ensemble d'information disponible à la date $t$. $I_t$ contient tout l'historique de la variable $Y_t$ et du terme d'erreur $\varepsilon_t$ jusqu'à la date $t$ incluse.

# Code

```{r}
#| message: false
#| warning: false
# Exécute le script setup.R pour charger tous les éléments importants
source(here::here("02-codes", "utils", "setup.R"))
```

## Données et analyse

### Présentation des données

Nous allons essayer de modéliser et de prédire la série du S&P 500 que l'on peut obtenir depuis 2007 grâce au package `pdfetch` qui est un package permettant de télécharger des séries de données depuis de nombreuses sources différentes. Les données sont retournées comme étant des objets `xts`. Il s'agit d'un type d'objet indiquant que la série est une série temporelle. Il est parfois nécessaires pour certaines fonctions que les séries données soient dans un format temporelle à cause de la gestion des dates.

```{r}
# Importer les données du S&P500
sp_500 <- 
  pdfetch::pdfetch_YAHOO(
    "^gspc", # Code de la série que l'on souhaite importer
    fields = "adjclose", # Variable que l'on souhaite importer
    from = as.Date("2007-01-03"), # Date de départ
    to = as.Date("2026-01-10") # Date de fin
  )
```

```{r}
# Visualiser les données
sp_500
```

```{r}
# Type des données
class(sp_500)
```

```{r}
# Représenter la série du S&P500
sp_500 |> 
  plot()
```

Visuellement, la série ne semble pas stationnaire. Elle semble être caractérisée par une tendance à la hausse et potentiellement une variance non constante.

```{r}
# Auto-corrélation des prix du S&P500
sp_500 |> 
  acf()
```

La fonction d'auto-corrélation montre une très forte persistance de l'auto-corrélation ce qui est un signe de non-stationnarité.

### Test ADF automatique

On va tester la non-stationnarité avec le test ADF. J'utilise ici, une fonction d'un package fait maison qui permet de faire la procédure ADF automatiquement (à l'inverse du package `urca`) et qui permet si nécessaire de retourner un dataframe avec les résultats, ce qui peut être pratique lorsque l'on veut tester la stationnarité de plusieurs séries et récupérer les résultats dans une table.

```{r}
# N'exécuter que si le package dobby est installé
if (rlang::is_installed("dobby")) {
  # Test ADF automatique
  dobby::adf_test_auto(
    sp_500, # Série à utiliser
    "S&P500", # Nom à donner à la série
    lags = 20, # Nombre de lags maximum à tester
    return_res = FALSE # Indiquer si on veut retourner un tableau de résultat ou juste un message
  ) 
}
```

La série n'est pas stationnaire selon le test ADF puisque l'on ne peut pas rejeter l'hypothèse nulle.

Le modèle sélectionné est le modèle sans constante ni tendance. La série du S&P500 est une une série de type DS. Il faut donc la différencier.

### Test ADF manuel

On peut également faire le test plus manuellement avec le package `urca` et sa fonction `ur.df()`. Il s'agit de la fonction qui est utilisée dans le package `dobby`. Simplement la fonction `ur.df()` ne donne pas de résultats de manière immédiate. Il faut regarder manuellement la significativité des termes voulus et tester chaque modèle un à un.

```{r}
urca::ur.df(
  y = sp_500, # Série à utiliser
  type = "trend", # type de modèle ADF : ici avec tendance et constante
  lags = 10, # nombre de lags maximum
  selectlags = "AIC" # Sélection du nombre de lag avec le critère AIC
) |> 
  summary() # Donner un résummé du résultat
```

On commence d'abord par sélectionner le modèle à utiliser : avec constante et tendance, avec constante sans tendance ou sans constante ni tendance. On commence toujours par le modèle avec constante et tendance. Le paramètre de la tendance est donnée à la ligne `tt`. Attention, pour tester la significativité des paramètres, il faut utiliser les valeurs tabulées par Dickey-Fuller puisque les valeurs usuelles ne sont pas utilisables. Les valeurs critiques indiquées par le package `urca` sont les valeurs critiques pour le test ADF joint. En général on ne fait pas ce test car on ne sait pas quel paramètre est significatif ou non.

La valeur critique associée au coefficient de la tendance dans le modèle 3 et avec un nombre d'observations grand au seuil de 5% est $2.78$. La t-value du coefficient de la tendance est de $1.808 < 2.78$. On ne peut donc pas rejeter l'hypothèse nulle de non-significativité de la tendance. On rejette le modèle 3 et on passe au modèle avec constante et sans tendance.

```{r}
urca::ur.df(
  y = sp_500, # Série à utiliser
  type = "drift", # type de modèle ADF : ici sans tendance et avec constante
  lags = 10, # nombre de lags maximum
  selectlags = "AIC" # Sélection du nombre de lag avec le critère AIC
) |> 
  summary() # Donner un résummé du résultat
```

On tester la significativité du paramètre de la constante `(Intercept)`. La valeur critique associée à la constante dans le modèle 2, avec un nombre grand d'observations et un seuil de 5% est de $2.52$. La t-value associée au paramètre de la constante est de $|-0.41| < 2.52$. On ne peut donc pas rejeter l'hypothèse nulle de non significativité de la constante dans le modèle 2. On rejette ce modèle et on passe au modèle 1.

```{r}
urca::ur.df(
  y = sp_500, # Série à utiliser
  type = "none", # type de modèle ADF : ici sans tendance et sans constante
  lags = 10, # nombre de lags maximum
  selectlags = "AIC" # Sélection du nombre de lag avec le critère AIC
) |> 
  summary() # Donner un résummé du résultat
```

On va tester la significativité du terme auto-régressif en niveau `z.lag.1`. La valeur critique du coefficient pour le modèle 1, avec un grand nombre d'observations et un seuil de 5% est de $-1.95$. La t-value du coefficient est : $3.371 > -1.95$. On rappelle que sur ce test de racine unitaire, la règle de décision est inversée. On rejetera l'hypothèse nulle de non-stationnarité lorsque la t-value et inférieure à la valeur critique (**NE PAS PRENDRE LES VALEURS ABSOLUES ICI**).

Notre t-value étant supérieure à la valeur critique, on ne peut pas rejeter l'hypothèse nulle de non stationnarité de la série. Notre série est vraisemblablement caractérisée par un processus $I(1)$ avec au moins une racine unitaire. Pour pouvoir l'utiliser dans des modélisations classiques, il faut la stationnariser.

### Stationnarisation

Ici on va la log-différencier pour obtenir le changement en pourcentage de la série, autrement dit les rendements journaliers observés.

```{r}
# Calculer les rendements de la série
# Faire la log différence des données + retirer les valeurs manquantes
rend_sp_500 <- 
  na.omit((log(sp_500) - log(lag(sp_500))) * 100) 

rend_sp_500
```

```{r}
rend_sp_500 |> 
  plot()
```

La série des rendements semble stationnaire en moyenne. En revanche, il semble que la volatilité ne soit pas constante au cours du temps. On remarque même des clusters de volatilités.

```{r}
rend_sp_500 |> 
  acf()
```

La fonction d'auto-corrélation semble également indiquer que la série des rendements est une série stationnaire. On remarque qu'il y a de l'auto-corrélation significative jusqu'à l'ordre 15.

Malgré les "preuves" graphiques, nous allons tester la stationnarité des rendements.

```{r}
# N'exécuter que si le package dobby est installé
if (rlang::is_installed("dobby")) {
  # Test ADF automatique
  dobby::adf_test_auto(
    rend_sp_500, 
    "S&P500", 
    lags = 20, 
    return_res = FALSE
  ) 
}
```

L'hypothèse nulle est rejetée, la série des rendements semble donc stationnaire en moyenne.

### Description des rendements

On s'intéresse ensuite à la description statistique de notre série. On commence par observer la normalité de la série :

#### Normalité

```{r}
# Excuter uniquement si le package dobby est installé
if (rlang::is_installed("dobby")) {
  # tester la présence de skewness et de surplus de kurtosis
  dobby::check_univariate_normality(
    as.numeric(rend_sp_500), # Série : doit être un vecteur numérique   et pas une série temporelle
    "S&P500", # Nom à donner à la série
    return_output = FALSE # Faut-il retourner un tableau de résultats   ?
  ) 
}
```

Cette fonction exécute les fonctions `moments::agostino.test()` pour tester si la skewness de la série est statistiquement différente de 0 ou non ; `moments::anscombe.test()` pour tester si la kurtosis de la série est statistiquement différente de 3 ; `moments::jarque.test()` pour tester si la kurtosis et la skewness de la série sont conjointement différentes de 0 et 3 respectivement.

Les tests statistiques indiquent que notre série est biaisée à gauche avec une skewness négative. Cela indique que les pertes ont d'avantage d'impact que les gains. La série est également caractérisée par une kurtosis supérieure à 3 indiquant que la probabilité d'observer des valeurs extrêmes est bien plus élevée que dans une loi normale.

Le test joint indique que les moments de la série des rendements ne sont pas égaux aux moments de la loi normale. A priori nos rendements ne suivent pas une loi normale.

Si le package `dobby` n'est pas installé, il est possible de faire les tests un par un :

```{r}
moments::agostino.test(rend_sp_500)
```

```{r}
moments::anscombe.test(rend_sp_500)
```

```{r}
moments::jarque.test(as.numeric(rend_sp_500))
```

```{r}
mean(rend_sp_500)
```

La moyenne de la série est proche de 0 comme le veut la théorie des marchés financier comme quoi en moyenne le gain est nul.

#### Auto-corrélation

On regarde ensuite l'auto-corrélation de la série avec les tests de Ljung-Box et Box-Pierce avec des ordres d'auto-corrélation de 1 et 20. Le package maison `dobby` utilise la fonction `stats::Box.test()` mais permet, entre autre, d'avoir des résultats pouvant être extraits.

```{r}
# Exécuter uniquement si le package dobby est installé
if (is_installed("dobby")){
  # Test de Ljung-Box
  dobby::check_univariate_autocorr(
    rend_sp_500, # Série à tester
    "S&P500", # Nom à donner à la série
    test_type = "Ljung-Box", # Nom du test d'auto-corrélation
    return_output = FALSE # Faut-il retourner un tableau de résultats
  )
}
```

```{r}
# Exécuter uniquement si le package dobby est installé
if (rlang::is_installed("dobby")){
 # Test de Box Pierce
  dobby::check_univariate_autocorr(
    rend_sp_500, 
    "S&P500", 
    test_type = "Box-Pierce", 
    return_output = FALSE
  ) 
}
```

Les tests de Box-Pierce et Ljung-Box indiquent tous les deux que la série présente de l'auto-corrélation à l'ordre 1 et 20 pour de très faibles niveaux de risque. L'objectif va être d'essayer de modéliser au maximum cette autocorrélation.

## Modélisation AR(1)

Nous allons commencer par essayer de modéliser la série avec un $AR(1)$.

```{r}
ar_1 <- 
  forecast::Arima(
    rend_sp_500, # Série à modéliser
    order = c(1, 0, 0), # Ordres p, d, d (AR, I, MA) à utiliser
    include.mean = TRUE # Inclure une constante ou pas
  )

summary(ar_1)
```

Le modèle $AR(1)$ indique que les rendements dépendent de manière négative de leur valeur passé$-0.1225$. Ce coefficient est significativement différent de 0 : $|-0.1225/0.0143| \approx 8.57 > 1.96$. On passe donc le test de surdimmensionnement.

```{r}
# Créer un dataframe pour visualiser les données
# ar_1$x permet de récupérer les données utilisées pour la modélisation
# ar_1_fitted permet de récupérer les valeurs prédites de la série
tibble(
  real_value = as.numeric(ar_1$x), # Valeur réelle
  fitted_value = ar_1$fitted, # Valeur estimée
  t = time(ar_1$x) # Date
) |>
  # Pivoter en longueur pour avoir les valeurs dans une seule colonne
  # Permet de facilement gérer les couleurs et légendes avec ggplot2
  pivot_longer(
    cols = !t, # pivoter toutes les colones sauf la date
    names_to = "variable",
    values_to = "rendements"
  ) |> 
  # Garder uniquement les dernières observations pour plus de clarté
  filter(row_number() > 9000) |> 
  print(n = 10) |> 
  # Faire un graphique simple
  ggplot(aes(x = t, y = rendements, color = variable)) +
  geom_line() 
```

On remarque que l'on n'arrive pas bien à reproduire l'amplitude et la volatilité de la série.

```{r}
if (rlang::is_installed("dobby")) {
  dobby::check_univariate_autocorr(
    ar_1$residuals, 
    "résidus AR(1)",
    return_output = TRUE # Retourner graphiques + tableaux
  )[1] # Simplement garder le 1er élément -> les graphiques 
}
```

Le test d'autocorrélation nous indique que toute l'autocorrélation de long-terme n'a pas été prise en compte. Le modèle n'est donc pas valide.

```{r}
if (rlang::is_installed("dobby")) {
  dobby::check_univariate_normality(
    as.numeric(rend_sp_500), 
    "résidus AR(1)", 
    return_output = FALSE
  ) 
}
```

Comme prévu les résidus ne sont pas normaux. Cela est logique car rien dans la modélisation AR(1) ne permet de prendre en compte la non-normalité. De ce fait, on laissera pour les prochaines modélisations le test de normalité de côté.

On utilise le test de White pour tester l'homogénéité de la variance ainsi que le test ARCH :

```{r}
# test de white
whitestrap::white_test(
  ar_1 # Modèle dont les résidus sont à tester
)

# ARCH test
fDMA::archtest(
  as.numeric(ar_1$residuals**2), # Série des résidus au carré
  lag = 20 # nombre de lags à utiliser
)
```

L'hypothèse d'homogénéité de la variance est rejetée. La variance des résidus ne semble pas constante dans le temps et semble être conditionnellement autocorrélée.

La modélisation AR(1) ne semble donc pas suffisante pour modéliser correctement notre série des rendements.

## Modélisation ARMA(p,q)

On peut utiliser la fonction `foreast::auto.arima()` pour déterminer le modèle qui correspond le mieux à nos données selon les critères d'informations. Cette fonction va déterminer le nombre de lags nécessaire pour modéliser notre série selon un critère d'information

```{r}
# Meilleur modèle selon les critères d'informations
best_arma <- 
  forecast::auto.arima(rend_sp_500) |> 
  print()
```

Il semble que selon le critère AIC, le meilleur modèle soit un $ARMA(4,5)$. On va tester si les derniers coefficients sont significatifs ou non

```{r}
# Tester la significativité des coefficients
# Créer un tibble contenant les coefficients
tibble(
  variable = names(best_arma$coef), # Extraire le nom des coefficients
  coef = best_arma$coef, # Extraire la valeur des coefficients
  std = sqrt(diag(best_arma$var.coef)) # Extraire l'écart-type des coefficients à partir de la matrice de variance-covariance
) |> 
  mutate(
    t_stat = abs(coef / std) # Calculer la t-stat de chaque coef
  )
```

On remarque que tous nos coefficients passent le test de surdimmensionnement puisque toutes les t-stats sont supérieures à 1.96. le modèle est donc validé de ce point de vue. (Rappel que le test de surdimmensionnement ne s'applique en général que sur les derniers coefficients de chaque composante).

```{r}
dobby::check_univariate_autocorr(best_arma$residuals, "residus")[1]
```

La majorité de l'autocorrélation de la série semble avoir été prise en compte. Il n'y a pas à court-terme d'auto-corrélation. Par contre, on peut voir que le résultat est moins net à plus long-terme. Ici à l'ordre 20, on rejette l'hypothèse nulle de non-significativité à 5%.

La fonction `Box.Ljung.Test()` permet de tracer un graphique indiquant la p.value du test de Ljung-Box pour tous les horizons voulus. On remarque que l'on ne capte pas toute l'auto-corrélation de long-terme avec notre modélisation.

```{r}
LSTS::Box.Ljung.Test(best_arma$residuals, lag = 40)
```

```{r}
# test de white
whitestrap::white_test(best_arma)

# ARCH test
fDMA::archtest(as.numeric(best_arma$residuals**2), lag = 20)
```

Encore une fois, nos résidus présentent des caractéristiques de variance non constante et de variance conditionnelle non-constante. Cela est normal puisque la modélisation ARMA ne permet pas de prendre en compte ce genre d'effets.

```{r}
# Créer un dataframe pour visualiser les données
tibble(
  real_value = as.numeric(best_arma$x), # Valeur réelle
  fitted_value = best_arma$fitted, # Valeur estimée
  t = time(best_arma$x) # Date
) |>
  # Pivoter en longueur pour avoir les valeurs dans une seule colonne
  # Permet de facilement gérer les couleurs et légendes avec ggplot2
  pivot_longer(
    cols = !t,
    names_to = "variable",
    values_to = "rendements"
  ) |> 
  # Garder uniquement les dernières observations pour plus de clarté
  filter(row_number() > 9000) |>
  ggplot(aes(x = t, y = rendements, color = variable)) +
  geom_line()
```

Encore une fois, la modélisation ARMA a du mal à reproduire l'amplitude de variation des rendements (ce qui est un cas classique).

## Modélisation de marche aléatoire

Nous allons par la suite comparer la puissance prédictive de nos modèle entre eux mais également nous allons chercher à voir s'ils permettent de mieux prédire et d'expliquer les rendements que le hasard donné par la marche aléatoire.

Rappel qu'une marche aléatoire consiste à explique la valeur actuelle par la totalité de sa valeur passée (coefficient auto-régressif de 1). Il s'agit d'un $ARIMA(0,1,0)$.

```{r}
rw <- Arima(rend_sp_500, order = c(0,1,0)) |> 
  print()
```

```{r}
# Créer un dataframe pour visualiser les données
tibble(
  real_value = as.numeric(rw$x), # Valeur réelle
  fitted_value = rw$fitted, # Valeur estimée
  t = time(rw$x) # Date
) |>
  # Pivoter en longueur pour avoir les valeurs dans une seule colonne
  # Permet de facilement gérer les couleurs et légendes avec ggplot2
  pivot_longer(
    cols = !t,
    names_to = "variable",
    values_to = "rendements"
  ) |> 
  # Garder uniquement les dernières observations pour plus de clarté
  filter(row_number() > 9000) |> 
  # Créer le graphique
  ggplot(aes(x = t, y = rendements, color = variable)) +
  geom_line()
```

La marche aléatoire reproduit les variations mais avec un temps de retard puisque la valeur prédite en $t$ est simplement la valeur observée en $t-1$.

## Comparaison in-sample

On peut comparer les modèles à partir de leur capacité à correctement reproduire la variation des données au sein de l'échantillon. Pour cela on extrait les séries des résidus de chaque modèle puis on va calculer des critères afin de déterminer le modèle qui minimise le plus possible une fonction de perte.

La fonction `purrr::map()` et ses dérivées prennent comme argument un vecteur/liste contenant différents éléments et une fonction qui va être appliquée sur chacun des éléments du vecteur/liste. Ici, chaque élément de la liste correspond à une série de résidus. On va appliquer une fonction de perte sur chacune de ces séries de résidus.

```{r}
# Pour chaque série de résidus, calculer le root mean square error
models_RMSE <- 
  map_dbl(
    list(rw$residuals, ar_1$residuals, best_arma$residuals), 
    \(epsilon) {sqrt(mean(epsilon**2))}
  )

# Pour chaque série de résidus, calculer le mean absolute error
models_MAE <- 
  map_dbl(
    list(rw$residuals, ar_1$residuals, best_arma$residuals), 
    \(epsilon) {mean(abs(epsilon))}
  )

# Faire une tibble contenant les différents critères pour chaque modèle
# AIC() extrait la valeur du critère AIC du modèle
tibble(
  model = c("rw", "AR(1)", "Best ARMA"),
  AIC = c(AIC(rw), AIC(ar_1), AIC(best_arma)),
  BIC = c(BIC(rw), BIC(ar_1), BIC(best_arma)),
  RMSE = models_RMSE,
  MAE = models_MAE
)
```

Les critères de sélection de modèle semblent indiquer que l'ARMA(4,5) donne de meilleurs résultats au sein de l'échantillon sur les critères des résidus. Mais quand on pondère sa capacité explicative par le nombre de paramètres à estimer, il ne fait pas forcément mieux que l'AR(1). Mais cela semble quand même assez marginal. Par contre il reproduit mieux les données que la marche aléatoire.

## Prévision et performance out-of-sample

Les modèles ARMA peuvent être utilisés pour faire de la prédiction. A partir des informations et du modèle estimé à la date $t$, on peut chercher à prédire pour $h$ périodes.

```{r}
# Prédire les 10 prochains rendements
forecast::forecast(
  best_arma, # Modèle utilisé pour prédire
  h = 10 # Horizon de prévision. Ici h = 10
)
```

Une façon de juger de la qualité d'un modèle est de tester sa capacité prédictive contre celle d'autres modèles. Pour cela, il faut tester le modèle out-of-sample. C'est à dire qu'il faut estimer le modèle sur un échantillon de test, prédire la prochaine valeur et la comparer avec la vraie valeur observée. Cela donne une erreur de prévision. Ensuite on ajoute la vraie valeur à l'échantillon et on répète le processus. On peut ensuite calculer la MAE ou RMSE de prévision du modèle et sélectionner le modèle qui en moyenne fait le moins d'erreurs.

Avec cette méthode on ne compare plus la capacité des modèles à correctement reproduire les données (in-sample), on compare la capacité des modèles à effectuer correctement des prévisions pour l'avenir sans connaître les informations futures (out-of-sample).

Pour faire cela on va créer ne fonction `func_predict()` qui va prendre comme argument une série temporelle, l'ordre de l'ARIMA à estimer et un scalaire qui va indiquer le nombre d'observations du premier échantillon utiliser pour prédire. On va ensuite utiliser cette fonction au sein de la fonction `runner::runner()` qui est une fonction qui va répéter une fonction voulue en faisant glisser ou en ajoutant une par une les observations d'un jeu de données.

Si le paramètre $k$ de `runner()` est égal à 0 (ou n'est pas renseigné), alors on ajoute une observation à chaque fois en gardant toutes les anciennes observations (on fait du récursif) (voir première image). Si $k$ est un scalaire différent de 0, alors tant que le nombre d'observations est inférieur à k, on continue d'ajouter la nouvelle observations en gardant les anciennes. Quand le nombre d'observations devient égal à $k$ alors on ajoute l'observation suivante tout en retirant la première observation possible (on fait de la fenêtre glissante) (voir deuxième image).

![](../images/runner1.png)

![](../images/runner2.png)

```{r}
# Fonction pour prédire à l'horizon de 1 jour et retourner un dataframe
func_predict <- function(serie, order = c(0,0,0), in_sample) {
  # Condition : Ne pas estimer le modèle si le nombre d'observations de la série est inférieur à un scalaire donné -> estimer sinon
  if (length(serie) > in_sample) {
    # Estimer le modèle
    model <- forecast::Arima(serie, order = order, include.mean = TRUE) 
  
    # Prédire à l'horizon de 1 jour
    forecast_value <- forecast::forecast(model, h = 1) 
  
    # Transformer le résultat en dataframe afin de pouvoir grouper les résultats
    df_forecast <- 
      # Prendre la valeur prédite
      forecast_value |> 
      # Créer une tibble et inclure une variable "obs" qui prendra la valeur de l'index
      as_tibble(rownames = "obs") |> 
      # Obtenir des noms corrects pour les variables
      janitor::clean_names() |> 
      # Sélectionner les variables d'intérêt
      select(obs, point_forecast)
    
    # Renvoyer le tibble créé
    return(df_forecast)
   } 
}
```

Pour récupérer automatiquement l'ordre de notre meilleur ARMA estimé, on va aller chercher l'élément `best_arma$arma` qui nous renvoi un vecteur contenant toutes les informations sur les paramètres de notre ARMA. Les ordres $p$ et $q$ sont données par les éléments numéros 1 et 2, tandis que l'ordre $d$ est donnée par le 6ème élément.

```{r}
best_arma$arma
```

```{r}
# Faire la prédiction out-of-sample pour le meilleur arma trouvé. 
# in_sample = 4750 : on n'estime le modèle qu'une fois que l'on a au moins 4750 observations. 
pred_out_sample_best_arma <- 
  # Runner applique 
  runner::runner(
    x = rend_sp_500, # Série à utiliser
    # Fonction à utiliser : celle que l'on vient de créer
    f = \(y) func_predict(y, order = best_arma$arma[c(1,6,2)], in_sample = 4750),
    k = integer(0) # On veut faire du récursif
  ) |> 
  # Runner renvoit une liste que l'on va lier en une seule tibble
  list_rbind() |> 
  mutate(obs = as.numeric(obs)) |> 
  print()
```

```{r}
# Faire la prédiction ouf-of-sample de l'AR(1)
pred_out_sample_ar_1 <- 
  runner::runner(
    x = rend_sp_500,
    f = \(y) func_predict(y, order = c(1,0,0), in_sample = 4750)
  ) |> 
  list_rbind() |> 
  mutate(obs = as.numeric(obs))
```

```{r}
# Faire la prédiction our-of-sample de la marche aléatoire
pred_out_sample_rw <- 
  runner::runner(
    x = rend_sp_500,
    f = \(y) func_predict(y, order = c(0,1,0), in_sample = 4750)
  ) |> 
  list_rbind() |> 
  mutate(obs = as.numeric(obs))
```

Une fois que l'on a effectué les prévisions out-of-sample, il suffit de les comparer avec les vraies valeurs puis de calculer les critères de perte.

```{r}
# Créer une tibble contenant les erreurs de prévision
df_error_pred <- 
  # Prendre la série des vraies valeurs des rendements
  rend_sp_500 |> 
  # La transformer en une tibble
  as_tibble() |> 
  # Obtenir des noms corrects pour les variables
  clean_names() |> 
  # Ajouter une variable "obs" qui correspond au numéro de ligne 
  # Permet de matcher avec les prédictions faites (on avait indiquer le numéro de l'index dans les résultats)
  mutate(obs = row_number()) |> 
  # Joindre à ce tibble les prévisions faites pour le meilleur arma
  # On join par la droite : on ne garde que les valeurs pour lesquelles on a fait une prédiction -> on enlève les 4750 premières observations qui n'ont pas de prédictions associées
  right_join(
    pred_out_sample_best_arma |> rename(point_forecast_best_arma = point_forecast),
    # Indiquer les variables qui contiennent les indices permettant d'associer les différentes lignes entre elles
    join_by(obs == obs)
  ) |> 
  # Joindre à ce tibble les prévisions faites pour l'AR(1)
  right_join(
    pred_out_sample_ar_1 |> rename(point_forecast_ar_1 = point_forecast),
    join_by(obs == obs)
  ) |> 
  # # Joindre à ce tibble les prévisions faites pour la marche aléatoire
  right_join(
    pred_out_sample_rw |> rename(point_forecast_rw = point_forecast),
    join_by(obs == obs)
  ) |> 
  # Calculer les erreurs de prévision
  mutate(
    error_pred_best_arma = gspc - point_forecast_best_arma,
    error_pred_ar_1 = gspc - point_forecast_ar_1,
    error_pred_rw = gspc - point_forecast_rw
  ) |> 
  # Garder uniquement les erreurs de prédiction et l'index
  select(obs, starts_with("error_pred")) |> 
  print()
```

```{r}
# Calculer la MAE et la RMSE pour les prédictions ou-of-sample
df_error_pred |> 
  # Pivoter les erreurs de prédictions pour les mettre dans une seule colonne : plus facile à faire des manipulations globales
  pivot_longer(
    cols = !(obs), # On fait tout pivoter sauf la colonne "obs"
    names_to = "model", 
    values_to = "error_pred"
  ) |> 
  drop_na() |> # On retire les valeurs manquantes s'il y en a 
  summarize(
    .by = model, # On groupe par type de model : Un critère par modèle
    RMSE = sqrt(mean(error_pred**2)),
    MAE = mean(abs(error_pred))
  )
```

Notre $ARMA(4,5)$ est le modèle qui permet le mieux en moyenne de prédire de façon out-of-sample. Attention cela ne veut pas dire qu'il prédit bien. On l'a juste comparer à d'autres modèles et sur une très petite période pour éviter les longs temps de computation. Il est fort probable au vu des valeurs prédites en échantillon que ce modèle prédise très mal les rendements. Cela semble en ligne avec la théorie des marchés financiers qui veut que les rendements soient imprévisibles.
