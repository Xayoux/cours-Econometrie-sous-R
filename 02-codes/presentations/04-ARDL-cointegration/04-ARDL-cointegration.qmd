---
title: "ARDL et cointégration"
author: "Romain CAPLIEZ"
format:
  html:
    embed-resources: true
    code-overflow: wrap
    toc: true
    toc-location: left
    toc-depth: 5
    toc-expand: 2
    include-in-header:
      text: |
        <style>
          p {
            text-align: justify
          }
        </style>
  pdf:
    include-in-header:
      text: |
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines=true, breaksymbol=, commandchars=\\\{\}}
        \DefineVerbatimEnvironment{verbatim}{Verbatim}{breaklines=true, breaksymbol=,commandchars=\\\{\}}
editor: visual
---

```{r}
#| eval: false
#| include: false
# A UTILISER POUR RENDER EN HTML ET PDF
quarto::quarto_render(
    here::here(
        "02-codes", 
        "presentations",
        "04-ARDL-cointegration", 
        "04-ARDL-cointegration.qmd"
    ), 
    output_format = "all"
)
```

# Théorie

Comme vu au cours du chapitre 3, une série temporelle peut être expliquée par ses propres valeurs passées et les chocs quelle a subi au cours du passé. Cependant, compter uniquement sur l'information disponible par cette série peut être limité. Ajouter de l'information supplémentaire peut être utile afin d'étudier des liens de causalité ou bien afin d'améliorer la qualité de la prévision effectuée.

Il est possible d'enrichir les processus autoprojectifs en intégrant des informations apportées par des variables exogènes.

## Hypothèses classiques des MCO en séries temporelles

Tout comme pour les données en coupe transversales, il existe des hypothèses permettant d'assurer que les estimateurs MCO en séries temporelles soient sans biais et de variance minimale tout en permettant de mener une analyse d'inférence.

-   **Hypothèse TS.1** **:** **Linéarité des paramètres**

Le processus stochastique $\{(x_{t,1}, x_{t,2}, \dots, x_{t,k}, y_t)\}$ : $t=1, 2, \dots, n$ suit un modèle linéaire dans ses paramètres :

$$
y_t = \alpha + \beta_1 X_{1t} + \dots + \beta_k X_{kt} + u_t
$$

Où $u_t$ correspond à la série des erreurs et $n$ le nombre de périodes temps (observations) de l'échantillon.

-   **Hypothèse TS.2 : Absence de colinéarité parfaite**

Dans l'échantillon (et donc dans le processus temporel sous-jacent), aucune variable explicative n'est une constante ou une combinaison linéaire parfaite d'autres variables explicatives.

Cette hypothèse autorise une certaine corrélation entre les variables explicatives tant que celles-ci ne sont pas parfaitement corrélées dans l'échantillon.

Si cette hypothèse n'est pas vérifiée le modèle n'est pas identifiable.

-   **Hypothèse TS.3 : Espérance conditionnelle nulle**

Pour chaque $t$, l'espérance mathématique du terme d'erreur $u_t$ compte tenu des variables explicatives pour toutes les périodes est égale à 0.

$$
E(u_t|\mathbf{X}) = 0, \hspace{0.2cm} t = 1, 2, \dots, n
$$

Cette hypothèse indique que l'erreur à l'instant $t$ ne doit pas être corrélée de quelque manière que ce soit avec les variables explicatives pour chaque période de temps. Les variables doivent être strictement exogènes. Ce qui est une hypothèse plus stricte que la seule exogénéité contemporaine $E(u_t|\mathbf{X}_t) = 0, \hspace{0.2cm} t = 1, 2, \dots, n$.

Sous les hypothèses TS.1 à TS.3 les estimateurs MCO en série temporelle sont sans biais.

-   **Hypothèse TS.4 : Homoscédasticité**

Conditionnellement à $X$, la variance de $u_t$ est la même pour tout $t$ : $Var(u_t | X) = Var(u_t) = \sigma^2 \text{ , } t = 1, 2, \dots, n$. Cela signifie que la variance conditionnelle des erreurs ne doit pas dépendre des variables explicatives (il suffit pour cela que $u_t$ et $X$ soient indépendants) et que la variance des erreurs soit constante au cours du temps.

-   **Hypothèse TS.5 : Absence d'autocorrélation**

Conditionnellement à $X$, les erreurs à deux périodes de temps différentes ne sont pas corrélées entre elles: $Corr(u_t, u_s) = 0 \hspace{0.2cm} \forall \hspace{0.2cm} t \neq s$. Si cette propriété n'est pas vérifiée, on dit que les erreurs souffrent d'autocorrélation ou de corrélation sérielle car elles sont corrélées dans le temps. L'autocorrélation implique que si à une période $t$ le niveau réel de la variable est pus élevé que la niveau attendu, alors il y a de plus fortes chances que ce soit également le cas dans la période suivante.

Sous les hypothèses TS.4 à TS.5, les erreurs sont dites sphériques. Dans ce cas, la variance des estimateurs MCO est minimale.

**Sous les hypothèses TS.1 à TS.5, les estimateurs MCO sont les meilleurs estimateurs linéaires possibles sans biais, conditionnellement à** $X$**.**

-   **Hypothèse TS.6 : Normalité**

Les erreurs $u_t$ sont indépendantes de $X$ et indépendamment et identiquement distribuées selon une loi normale $(0, \sigma^2)$. Cette hypothèse implique les hypothèses TS.3, TS.4, TS.5 mais est plus forte de par les hypothèses d'indépendance et de normalité.

Sous les hypothèses TS.1 à TS.6, les estimateurs des MCO sont normalement distribués conditionnellement à $X$. De plus, sous l'hypothèse nulle, chaque statistique $t$ suit une distribution de Student, et chaque statistique $F$ suit une loi de Fisher.

## Propriétés asymptotiques des MCO

Les hypothèses classiques des MC0 en séries temporelles peuvent être difficiles à remplir. En effet, elles imposent généralement que ces hypothèses soient vérifiées pour toutes les périodes de temps simultanément ce qui est difficile à prouver. Cependant, grâce aux séries dites faiblement dépendantes (stationnaires), nous pouvons assouplir ces hypothèses et utiliser les propriétés asymptotiques des MCO.

-   **Hypothèse TS.1' : Linéarité et faible dépendance**

Le processus stochastique $\{(x_{t,1}, x_{t,2}, \dots, x_{t,k}, y_t)\}$ : $t=1, 2, \dots, n$ suit un modèle linéaire dans ses paramètres :

$$ y_t = \alpha + \beta_1 X_{1t} + \dots + \beta_k X_{kt} + u_t $$

Où $u_t$ correspond à la série des erreurs et $n$ le nombre de périodes temps (observations) de l'échantillon. Ce processus est un processus stationnaire et de faible dépendance. La Loi des Grands Nombres et le Théorème Central Limite s'appliquent aux moyennes de l'échantillon.

La stationnarité de la série n'est pas critique pour les propriétés asymptotiques des MCO (il faut une forme de stabilité dans les temps des relations). La restriction additionnelle majeure de cette hypothèse est l'hypothèse de faible dépendance qui implique que $X_t$ et $X_{t+h}$ sont "presque indépendants" lorsque $h$ augmente.

-   **Hypothèse TS.2' : Absence de parfaite colinéarité**

Cette hypothèse est la même que l'hypothèse TS.2 :

Dans l'échantillon (et donc dans le processus temporel sous-jacent), aucune variable explicative n'est une constante ou une combinaison linéaire parfaite d'autres variables explicatives.

Cette hypothèse autorise une certaine corrélation entre les variables explicatives tant que celles-ci ne sont pas parfaitement corrélées dans l'échantillon.

Si cette hypothèse n'est pas vérifiée le modèle n'est pas identifiable.

-   **Hypothèse TS.3' : Moyenne conditionnelle nulle**

Les variables explicatives sont simultanément exogènes : $E(u_t|X_t) = 0$.

Par stationnarité, on suppose que si l'exogénéité est vérifiée pour une période de temps, cela est vrai aussi pour toutes les périodes de temps. Supprimer la stationnarité nous demanderait de vérifier que la condition est vraie pour tout $t=1, 2, \dots, n$. Cette hypothèse est bien plus facile à vérifier que l'hypothèse TS.3 qui suppose que le terme d'erreur en $t$ n'est corrélé d'aucune manière que ce soit à toutes les variables explicatives pour toutes les périodes de temps. Ici, il faut simplement vérifier qu'il n'y ait pas de lien quelconque à la période $t$. Attention cela ne signifie pas que cette hypothèse supprime la possibilité pour que le terme d'erreur contienne des variables retardées. Dans ce cas il faut les prendre en compte dans le modèle.

**Sous les hypothèses TS.1', TS.2' et TS.3', les estimateurs MCO sont consistants :** $\text{plim }\hat{\beta} = \beta$**.**

-   **Hypothèse TS.4' : Homoscédasticité**

Les erreurs sont simultanément homoscédastiques : $Var(u_t|X_t) = \sigma^2$

Une foi encore, la stationnarité nous permet de ne vérifier que la relation contemporaine et pas pour toutes les valeurs de temps.

-   **Hypothèse TS.5' : Absence d'autocorrélation**

Pour tout $t \neq s$, $E(u_t u_s |X_t X_s) = 0$. Cette hypothèse signifie simplement que les termes d'erreurs de périodes de temps différentes ne doivent pas être corrélés.

**De manière asymptotique (lorsque le nombre d'observation croit vers l'infini), sous les hypothèses TS.1' à TS.5', les estimateurs MCO sont asymptotiquement normalement distribués. De plus les écarts-types, t-stat, F-stat et LM-stat issus des MCO sont asymptotiquement valides.**

Les modèles ayant des variables explicatives avec tendance peuvent satisfaire les hypothèses TS.1' à TS.5' à condition que ces variables soient stationnaires en tendance (processus Trend Stationnary). Aussi longtemps que les tendances sont intégrées dans l'équation si cela est nécessaire, la procédure d'inférence usuelle est asymptotiquement valide.

## Modèles à Retards Distribués (DL)

Les modèles à retards distribués sont des modèles dynamiques de séries temporelles dans lesquels la dynamique de la variable $Y$ peut être expliquée par des valeurs contemporaines et/ou retardées des variables explicatives $X$.

Ce type de modèle permet d'incorporer une dynamique plus riche et d'obtenir des effets marginaux (parfois causaux) des variables $X$ sur la variable $Y$.

Suivant si l'on considère un nombre fini ou infini de valeurs retardées pour la variable explicative, on parlera de modèles à retards distribués finis ou infinis. En pratique seuls les modèles à retards finis peuvent être estimés.

Un modèle à retards distribués finis d'ordre $DL(q_1, \dots, q_k)$ s'écrit sous sa forme générale :

$$
Y_t = \alpha + \sum_{i = 1}^k \sum_{j=0}^{q_j} \beta_{ij} X_{i, t-j} + \varepsilon_t
$$

Chaque variable $X$ peut avoir un nombre de retards qui lui est propre afin de capter au mieux la dynamique de la série $Y$.

Ces modèles souffrent souvent d'une forte multicolinéarité puisque les retards d'une variable $X_{it}$ sont souvent corrélés entre eux, ce qui réduit la précision des estimateurs.

Il est également courant que ce type de modèle n'arrive pas à capter toute la dynamique de la variable dépendante $Y_t$, à moins d'intégrer de nombreux retards pour chaque variable $X$. Les résidus restent souvent auto-corrélés rendant la variance des estimateurs non-minimale.

## Multiplicateurs et effets marginaux

### Dans un modèle statique

L'effet marginal d'une variable $X$ sur $Y$ est facilement déterminable dans le cadre d'un modèle statique (toutes les variables sont prises à la même date). Ainsi dans le modèle suivant :

$$ Y_t = \alpha + \beta X_t + \varepsilon_t $$

L'effet marginal de $X_t$ sur $Y_t$ (l'impact d'une augmentation d'une unité de $X$ sur $Y$) est donné par :

$$ \frac{\partial Y_t}{\partial X_t} = \beta $$

La réponse de la variable $Y_t$ à un changement d'une unité de la variavle $X_t$ est supposée être immédiate et complète à la fin de la période de mesure.

### Dans un modèle à retards distribués

Considérons le modèle à retards distribués d'ordre 2 suivant :

$$
Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \beta_2 X_{t-2} + u_t
$$

Pour interpréter, les coefficients de cette équation, supposons que $X$ soit constant et égal à $c$ (on se trouve sur une relation d'équilibre). On suppose une augmentation temporaire de $X$ d'une unité à la période $t$. On a donc :

$$
X_{t-2} = c \hspace{0.2cm}; X_{t-1} = c \hspace{0.2cm}; X_{t} = c +1 \hspace{0.2cm}; X_{t+1} = c \hspace{0.2cm}; X_{t+2} = c
$$

Afin de mettre l'accent sur l'effet toute chose égale par ailleurs, on considère que $u_t = 0$. On a :

$$
\begin{aligned}
Y_{t-1} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c \\
Y_t &= \alpha + \beta_0 \left( c+1 \right) + \beta_2 c + \beta_3 c \\
Y_{t+1} &= \alpha + \beta_0 c + \beta_1 \left( c+1 \right) + \beta_3 c \\
Y_{t+2} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 \left( c+1 \right) \\
Y_{t+3} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c
\end{aligned}
$$

En considérant les deux premières équations, on obtient : $Y_t - Y_{t-1} = \beta_0$ où $\beta_0$ représente le changement immédiat de $Y$ suite à une hausse de l'unité $X$ à la période $t$, ce que l'on peut noter $\frac{\partial Y_t}{\partial X_t}$ et est communément appelé le multiplicateur de court-terme.

De la même manière $Y_{t+1} - Y_{t-1} = \frac{\partial Y_{t+1}}{\partial X_t} = \beta_1$ et correspond à la variation de $Y$ une période après le changement temporaire de $X$.

$Y_{t+2} - Y_{t-1} = \frac{\partial Y_{t+2}}{\partial X_t} = \beta_2$ correspond à la variation de $Y$ une période après le changement temporaire de $X$. A la période $t+1$, $Y$ reprend sa valeur initiale car on a supposé que seuls deux retards de $X$ pouvaient avoir un impact sur $Y$. Ces multiplicateur sont aussi appelés *Delays multipliers*.

On peut également s'intéresser au changement de $Y$ à la suite d'une augmentation permanente de $X$. Considérons que $X$ augmente d'une unité de manière permanente à la période $t$ soit :

$$
X_{t-2} = c \hspace{0.2cm}; X_{t-1} = c \hspace{0.2cm}; X_{t} = c +1 \hspace{0.2cm}; X_{t+1} = c+1 \hspace{0.2cm}; X_{t+2} = c+1
$$

On a donc :

$$
\begin{aligned}Y_{t-1} &= \alpha + \beta_0 c + \beta_1 c + \beta_2 c \\
Y_t &= \alpha + \beta_0 \left( c+1 \right) + \beta_2 c + \beta_3 c \\
Y_{t+1} &= \alpha + \beta_0 \left( c+1 \right) + \beta_1 \left( c+1 \right) + \beta_3 c \\
Y_{t+2} &= \alpha + \beta_0 \left( c+1 \right) + \beta_1 \left( c+1 \right) + \beta_2 \left( c+1 \right)\end{aligned}
$$

A la suite de l'augmentation permanente de $X$, après une période $Y$ a augmenté de $\beta_0 + \beta_1$. Après deux périodes, $Y$ a augmenté de $\beta_0 + \beta_1 + \beta_2$. La somme des coefficients de $X$ et de ses deux retards représente la variation de long-terme de $XY$ suite à un changement permanent de $X$. On l'appelle le multiplicateur de long-terme. Les valeurs intermédiaires sont appelées *Interim multipliers*.

## Modèles Auto-Régressifs à Retards Distribués (ARDL)

Afin de rendre les modèles plus parcimonieux et essayer de capter la majeure partie de la dynamique de la variable dépendante, il est courant d'introduire une/des valeurs retardées de la variable $Y$ en plus des valeurs retardées des variables $X$. Cela permet de tenir plus facilement compte du caractère auto-corrélées de la majeure partie des séries temporelles étudiées en économie.

Sous sa forme générale, un $ARDL(p, q_1, \cdots, q_k)$ s'écrit :

$$
Y_t = \alpha + \sum_{l = 1}^p \phi_l Y_{t-l} + \sum_{i = 1}^k \sum_{j=0}^{q_i} \beta_{ij} X_{i, t-j} + \varepsilon_t
$$

De manière classique, $\varepsilon_t$ doit être un bruit blanc (non-auto-corrélé et de variance constante).

De la même manière que pour les modèles $ARMA$, il existe différentes façon (non-exclusives) de déterminer les retards $p$ et $q_i$ d'un modèle $ARDL$ :

-   Tester la significativité des paramètres $\phi_p$ et des différents $\beta_{i,q_i} \hspace{0.2cm} \forall \hspace{0.2cm} i = \{1, \dots, k\}$ .

-   Utiliser les critères d'informations (AIC, BIC,...) qui vont chercher la spécification qui permet le mieux de reproduire les données en ajoutant une contrainte de parcimonie.

-   Utiliser les tests d'autocorrélation des résidus (Ljung-Box) : augmenter l'ordre autorégressif jusqu'à ce que les résidus du modèle soient compatibles avec un bruit blanc.

Les modèles $ARDL$ peuvent être estimés à partir de la méthode des MCO.

## Cointégration

De manière générale, il n'est pas correct d'utiliser des variables non-stationnaires (caractérisées par un processus $I(1)$) dans des modèles de régression. En effet, lorsque de telles variables sont utilisées les estimateurs MCO ne sont généralement plus convergent. De plus lorsque deux variables $I(1)$ sont régressées l'une sur l'autre, il a été montré qu'une régression MCO indiquera souvent l'existence d'une relation statistique entre ces deux variables, même lorsqu'elles n'ont aucune raison d'être liées. Il s'agit du problème de *corrélation fallacieuse*.

Si $Y_t$ et $X_t$ sont des processus $I(1)$ aucunement liés entre eux, alors la régression suivante est fallacieuse et risque d'indiquer l'existence d'une relation qui n'existe pas :

$$
Y_t = \alpha + \beta X_t + u_t
$$

Généralement, pour contrer ce problème, on va stationnariser les variables en les prenant en première différence par exemple. Ainsi le modèle à estimer devient :

$$
\Delta Y_t = \gamma+ \delta\Delta X_t + e_t
$$

Avec $\Delta Y_t = Y_t - Y_{t-1}$. Mais cette régression n'est ps équivalente à la précente. $\gamma$ donne une information bien différente de $\beta$. La deuxième régression explique la différence de $Y$ en terme de différence de $X$ ce qui n'a rien à voir avec l'explication de $Y$ en fonction de $X$.

La notion de cointégration introduire par **Engle et Granger (1987)** permet dans certain cas de résoudre ce problème de régression fallacieuse.

En général, si $Y_t$ et $X_t$ sont des processus $I(1)$, alors $Y_t - \beta X_t$ sera un processus $I(1)$ pour n'importe quelle valeur de $\beta$.

Cependant, il est possible que pour une certaine valeur $\beta \neq 0$, alors $Y_t - \beta X_t$ soit un processus $I(0)$. Si un tel $\beta$ existe, alors on dira que $Y$ et $X$ sont cointégrés et $\beta$ sera appelé le paramètre de cointégration. Dans ce cas, les deux séries possèdent une relation de long-terme commune. Il peut y avoir des déviations par rapport à cet équilibre de long-terme, mais il existe des forces économiques qui vont ramener les deux variables vers leur relation d'équilibre. La différence entre les deux variables a tendance à revenir vers sa valeur moyenne. Dans ce cas, la régression entre $Y_t$ et $X_t$ peut se faire sans être fallacieuse.

En revanche, puisque $X_t$ est un processus $I(1)$, les procédures d'inférence usuelles ne s'appliquent pas nécessairement. De manière analogue aux modèles de régression linéaire, plusieurs hypothèses sont nécessaires à l'obtention d'une distribution normale du paramètre $\beta$ et d'une distribution exacte de la statistique de Student. La plupart peuvent se corriger en augmentant la taille de l'échantillon (normalité des résidus) ou en corrigeant les écarts-types obtenus (sphéricité des erreurs), ce n'est pas le cas pour l'hypothèse d'exogénéité stricte des régresseurs. Cette hypothèse implique dans le cas de séries temporelles fortement persistantes (dont font partis les processus $I(1)$) que les régresseurs $X$ ne soient liés d'aucune manière que ce soit avec les erreurs pour toutes les périodes de temps possibles. Il s'agit d'une hypothèse extrêmement forte qu'il est difficile de montrer sa validité en pratique.

Pour contrecarrer (en partie) ce problème, il est possible d'inclure $\Delta X_t$ ainsi que d'éventuels retards de cette variable dans l'équation de régression afin de purger la corrélation entre $X$ et $u$. Dans son cas le plus simple, la régression devient :

$$
Y_t = \alpha + \beta X_t + \Delta X_t + \varepsilon_t
$$

Le choix d'inclure ou non des retards additionnels est une question empirique qu'il convient de justifier.

## Modèles à correction d'erreur

Le concept de cointégration permet également d'enrichir les types de modèles dynamiques à notre disposition. Si $Y_t$ et $X_t$ sont des processus $I(1)$ non-cointégrés, alors la modélisation doit être une modélisation en différence première (généralement). Considérons le modèle ARDL suivant :

$$
\Delta Y_t = \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + u_t
$$

Dans le cas où $Y_t$ et $X_t$ sont cointégrés, nous disposons de variables supplémentaires que l'on peut inclure dans cette équation. En effet dans ce cas : $Y_t - \beta X_t$ est un processus $I(0)$, et ses retards peuvent sans problèmes être inclus dans une régression. L'équation devient donc :

$$
\begin{aligned}
\Delta Y_t &= \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta (Y_{t-1} - \beta X_{t-1}) + u_t \\
\Leftrightarrow \Delta Y_t &= \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta \text{ECT} + u_t
\end{aligned}
$$

Le terme $(Y_{t-1} - \beta X_{t-1})$ est appelé le terme à correcteur d'erreur (*Error Correction Term*), et ce type de modélisation est appelée Modèle à Correction d'Erreur.

Ce type de modèles permet d'étudier la dynamique de court terme dans la relation entre $Y$ et $X$. Si $\delta < 0$ et si $Y_{t-1} > \beta X_{t-1}$, alors $Y$ a dépassé l'équilibre durant la période précédente. Puisque $\delta < 0$, le terme à correction d'erreur fait en sorte que $Y$ est ramené vers l'équilibre en diminuant. A l'inverse, si $Y_{t-1} < \beta X_{t-1}$, alors $Y$ était inférieur à la valeur d'équilibre à la période précédente. Dans ce cas, le terme à correction d'erreur induit une variation positive de $Y$ vers l'équilibre.

Le temps que mettent les séries à revenir à leur équilibre est donné par $\frac{1}{\delta}$.

## Procédure d'Engle-Granger

La procédure d'Engle-Granger est une procédure en deux étapes pour estimer des modèles à correction d'erreur. Attention la procédure présentée ici n'est valide que pour deux séries toutes les deux caractérisées par un processus $I(1)$.

### 1ère étape : Tester l'existence de la cointégration

Dans un premier temps, si les séries $Y$ et $X$ sont des processus $I(1)$, alors il convient de régresser $Y$ sur $X$ :

$$
Y_t = \alpha + \beta X_t + e_t
$$

On récupère ensuite la série des résidus $\hat{e}_t$ et l'on teste la stationnarité de cette série. Généralement on utilise le test ADF avec un modèle sans constante mais avec ou sans tendance. On estime la relation :

$$
\Delta \hat{e}_t = \phi \hat{e}_{t-1} + \sum_{i=1}^p \phi_i \Delta \hat{e}_{t-i} + u_t
$$

avec $\hat{e}_t = \hat{Y}_t - \hat{\alpha} - \hat{\beta}X_t$. On teste ensuite :

$$
\left\{    
\begin{array}{ll}       
H_0 : \phi = 0 \text{ ; } \hat{e}_t \text{ non stationnaire}  \\       
H_1 : \phi < 0 \text{ ; } \hat{e}_t \text{ stationnaire}     
\end{array}
\right.
$$

Avec la statistique de test :

$$
t_{\hat{\phi}} = \frac{\hat{\phi}}{\hat{\sigma}_{\hat{\phi}}}
$$

Attention, les valeurs critiques pour tester la stationnarité de cette série sont différentes de celles utilisées classiquement.

| Type de test  |  1%   |  5%   |  10%  |
|:-------------:|:-----:|:-----:|:-----:|
| Sans tendance | -3.90 | -3.34 | -3.04 |
| Avec tendance | -4.32 | -3.78 | -3.50 |

La règle de décision est la suivante :

-   Si $t_{\hat{\phi}} < t$ : on rejette l'hypothèse nulle. Les résidus sont stationnaires. Les séries $Y_t$ et $X_t$ sont cointégrées.

-   Si $t_{\hat{\phi}} > t$ : on ne peut pas rejeter la l'hypothèse nulle. Les résidus ne sont pas stationnaires. Les séries $Y_t$ et $X_t$ ne sont pas cointégrées.

### 2ème étape : Estimer le modèle à correction d'erreur

Si $\hat{e}_t$ est bien un processus $I(0)$, alors les séries $Y_t$ et $X_t$ sont cointégrées et caractérisées par une relation de long terme. La régression de l'une l'autre n'est plus une régression fallacieuse. On peut donc estimer le modèle en niveau (en incluant $\Delta X_t$ et ses retards et/ou ses valeurs avancées ainsi que les retards de $Y_t$ si besoin) et obtenir les coefficients de long-terme de la série.

On peut ensuite estimer le modèle à correction d'erreur dans lequel le terme à correction d'erreur correspond à la série des résidus $\hat{e}_t$ :

$$
\Delta Y_t = \alpha + \phi \Delta Y_{t-1} + \gamma_0 \Delta X_t + \gamma_1 \Delta X_{t-1} + \delta \hat{e}_{t-1} + u_t
$$

Ce modèle nous permet d'obtenir des effets de court-terme avec les coefficients des variables en différence, ainsi que la vitesse d'ajustement des séries suite à un écart à leur relation de long-terme.

## Extension de la cointégration

La cointégration à la Granger peut évidemment être généralisée avec un nombre de variables supérieures à 2. Mais cette méthode requiert tout de même que l'ensemble des variables soient intégrées du même ordre. [**Pesaran et al (2001)**](https://onlinelibrary.wiley.com/doi/full/10.1002/jae.616?casa_token=PNv1tRySyY0AAAAA%3A871E331d0NLVpRpeJ4TghehXlefBXEznk49q2vbrkuWUMKenTGGqaPc_rK95X3aWp34rPcw8leVRkog) ont développé une méthodologie permettant de tester l'existence d'une relation entre les niveaux de $K$ variables que celles-ci soient intégrées d'ordre 1 et/ou d'ordre 0.

La méthodologie repose sur l'estimation de 3 modèles :

### Modèle ARDL général

La forme générale d'un $ARDL(p, q_1, \dots, q_k)$ est donnée par :

$$
Y_t = c_0 + c_1t + \sum_{i=1}^pb_{y,i}Y_{t-i} + \sum_{j=1}^k \sum_{l=0}^{q_j}b_{j,l}X_{j,t-l} + \varepsilon_t
$$

Ce modèle régresse la variable $Y_t$ sur une constante $c_0$, une possible tendance linéaire $c_1$, ses $p$ valeurs passées ainsi que les valeurs contemporaines et passées des variables $X_t$. Le choix des ordres $p, q_1, \dots, q_j$ est une question empirique.

Les multiplicateurs de court-terme sont donnés par les coefficients associés aux valeurs contemporaines des variables $X_{j,t}$. ces multiplicateurs indiquent la réponse de $Y_t$ suite à une variation temporaire d'une unité de $X_{j,t}$.

$$
\text{SR}_{\text{ARDL}} = \frac{\partial Y_t}{\partial X_{j,t}} = b_{j,0} \hspace{0.3cm} j \in \left\{ 1, \cdots, k \right\}
$$

Les multiplicateurs $\text{Delay}_{x_{j,s}}$ indiquent la réponse de la variable $Y_{t+s}$ suite à une variation temporaire de la variable $X_{j,t}$. Il s'agit d'une réponse directe. Il s'agit du coefficient associé au $s^{\text{ième}}$ retard de la variable $X_{j}$ auquel viennent s'ajouter les effets directs sur les périodes antérieures pondérées par les effet auto-régressifs de la variable $Y$.

$$
\text{Delay}_{x_{j,s}} = \frac{\partial Y_{t+s}}{\partial X_{j,t}} = b_{j,s} + \sum_{i=1}^{min \left\{ p,s \right\}} b_{y,i} \frac{\partial Y_{t+\left( s-i \right)}}{\partial X_{j,t}} \hspace{0.6cm} ; \hspace{0.3cm} b_{j,s} = 0 \hspace{0.3cm} \forall \hspace{0.3cm} s>q
$$

Les multiplicateurs $\text{Interim}_{x_{j,s}}$ indiquent la réponse de la variable $Y_{t+s}$ suite à une variation permanente de la variable $X_{j,t}$. Il s'agit de la somme des multiplicateurs $\text{Delay}_{x_{j,i}}$ avec $i = \{1, \dots, s\}$.

$$
\text{Interim}_{x_{j,s}} = \sum_{i=0}^s \text{Delay}_{x_{j,s}}
$$

Les multiplicateurs de long-terme indiquent la réponse totale de la variable $Y$ suite à un choc permanent sur la variable $X_{j,t}$. Il s'agit de la somme des coefficients associés à la variable $X_t$ et à ses retards divisé par 1 moins la somme des coefficients auto-régressifs de la variable $Y$.

$$
\text{LR}_{\text{ARDL}} = \frac{\partial Y_{t+\infty}}{\partial X_{j,t}} = \theta_j = \frac{\sum_{l=0}^{q_j} b_{j,l}}{1 - \sum_{i=1}^p b_{y,i}} \hspace{0.6cm} ; \hspace{0.3cm} j  \in  \left\{ 1, \cdots, k \right\}
$$

Le vecteur de coefficients $\hat{\theta}_j$ permet de construire l'équation de long-terme associé aux niveaux des variables :

$$
\hat{Y}_t^{\text{LT}} = \hat{\alpha} + \sum_{j=1}^k \hat{\theta}_j X_{j,t}
$$

### Modèle Unrestricted Error Correction Model (UECM)

La formule d'un modèle UECM dérivé du modèle ARDL précédent est la suivante :

$$
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t
$$

Ce modèle met en relation la première différence de la variable $Y_t$ avec les retards de sa première différence ainsi que les retards de la première différence des variables $X_t$ comme un $ARDL$ classique avec des variables en première différence. Les coefficients associés à ces variables en différences premières représentes la relation de court-terme entre $Y$ et $X$. Cependant le modèle UECM va également inclure les niveaux des variable $Y$ et $X_j$ retardés d'une période afin d'inclure les relations de long-terme entre les variables.

A partir de ce modèle il est possible de calculer les multiplicateurs de court-terme et de long-terme. Ces multiplicateurs sont strictement égaux aux multiplicateurs calculés à partir du modèle $ARDL$ en niveaux.

Les multiplicateurs de court-terme sont donnés par les coefficients associés aux valeurs contemporaines des premières différences de $X_j$.

$$
\text{SR}_{\text{UECM}} = \text{SR}_{\text{ARDL}} = \frac{\partial Y_t}{\partial X_{j,t}} = \omega_j \hspace{0.6cm} j \in \left\{ 1, \cdots, k \right\}
$$

Les multiplicateurs de long-terme sont donnés par la division du coefficient associé à la variable $X_{j,t}$ en niveau avec le coefficient associé à $Y_t$ en niveau multiplié par moins 1.

$$
\text{LR}_{\text{UECM}} = \text{LR}_{\text{ARDL}} = \frac{\partial Y_{t+\infty}}{\partial X_{j,t}} = \theta_j = \frac{\pi_j}{-\pi_y} \hspace{0.6cm} j \in \left\{ 1, \cdots, k \right\}
$$

De la même manière que pour le modèle $ARDL$, le vecteur de coefficient de long-terme $\hat{\theta}_j$ peut être utilisé pour construire l'équation de long-terme des variables en niveau :

$$
\hat{Y}_t^{\text{LT}} = \hat{\alpha} + \sum_{j=1}^k \hat{\theta}_j X_{j,t}
$$

Ce modèle apporte une précision supplémentaire par rapport au modèle $ARDL$ puisqu'il indique également la vitesse de convergence des séries suite à un écart à la relation d'équilibre à travers le coefficient $\pi_y$. En effet, si $Y_{t-1}$ augmente de 1 tandis que toutes les variables restent inchangées, cela signifie que $Y_{t-1}$ s'est écarté de sa valeur de long-terme. Le coefficient $\pi_y$ indique la force de rappel qui va ramener $Y_t$ vers sa valeur de long-terme. $\frac{1}{\pi_y}$ indique le temps nécessaire pour $Y$ retourne à sa valeur de long-terme suite à un déséquilibre.

### Modèle Restricted Error Correction Model (RECM)

A partir du modèle $ARDL$ il est également possible de construire une modélisation $RECM$ de la forme suivante :

$$
\Delta Y_t = c_0 + c_1t + \sum_{i=1}^{p-1} \psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_{j-1}} \Delta X_{j, t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \pi_y \text{ECT}_t = \varepsilon_t
$$

Ce modèle relie la première différence de la variable $Y$ avec les premières différences (retardées) des variable $X_j$. La différence avec le modèle UECM réside dans le fait que les variables en niveau ont été retirées et inclues dans le terme $\text{ECT}_t$. cela permet de gagner des degrés de liberté puisqu'il y a moins de coefficients à estimer. On augmente donc la précision de nos estimateurs en diminuant leur variance estimée. Il permet également d'obtenir plus de flexibilité concernant l'inclusion de tendance ou de constante (voir plus loin).

Il faut remarquer que ce modèle permet d'obtenir les même coefficients de court-terme que les modèles $ARDL$ et $UECM$ à partir des coefficients $\omega_j$. La force de rappel donnée par le coefficient $\pi_j$ est également la même que dans le modèle $UECM$.

Lors de la construction de ce modèle, 5 cas sont possibles (et vont conditionner les tests statistiques) :

-   **Cas 1 : Sans constante ni tendance dans les équation de court- et long-terme**

Dans ce cas, on va contrainte qu'il n'y ait ni tendance ni constante à la fois dans l'équation en différence et dans le terme à correction d'erreur. On a donc :

$$
\begin{aligned}
&c_0 = c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{aligned}
$$

Le terme à correction d'erreur en $t$ est donné la différence entre la valeur de $Y_{t-1}$ et sa valeur de long-terme en $t-1$ (sans constante ni tendance) calculée à partir des multiplicateurs de long-terme $\theta_j$. Il s'agit de la valeur du déséquilibre de $Y$ avec sa valeur de long-terme.

-   **Cas 2 : Sans tendance ni constante dans l'équation de court-terme ; sans tendance mais avec constante dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il n'y ait ni constante ni tendance dans l'équation en différence, mais on va inclure une constante dans le terme à correction d'erreur. On a donc :

$$
\begin{aligned}
&c_0 = c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \mu + \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{aligned}
$$

-   **Cas 3 : Sans tendance mais avec constante dans l'équation de court-terme ; sans tendance et sans constante dans l'équation de long-terme**

Dans ce cas, on va contraire à ce qu'il y ait une constante mais pas de tendance dans l'équation en différence. On ne va inclure ni tendance ni constante dans le terme à correction d'erreur. On a donc :

$$
\begin{aligned}
&c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{aligned}
$$

-   **Cas 4 : Sans tendance mais avec constante dans l'équation de court-terme ; sans constante et avec tendance dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il y ait une constante mais pas de tendance dans l'équation en différence. On va inclure une tendance linéaire dans le terme à correction d'erreur mais pas de constante. On a donc :

$$
\begin{aligned}
&c_1 = 0 \\
\\
&\text{ECT}_t = Y_{t-1} - \left( \delta(t-1) + \sum_{j=1}^k \theta_j X_{j, t-1} \right)
\end{aligned}
$$

-   **Cas 5 : Avec tendance et constante dans l'équation de court-terme ; Sans tendance ni constante dans l'équation de long-terme**

Dans ce cas, on va contraindre à ce qu'il y ait une constante et une tendance dans l'équation en différence et à ce qu'il n'y ait ni constante ni tendance dans le terme à correction d'erreur. On a donc :

$$
\text{ECT}_t = Y_{t-1} - \left( \sum_{j=1}^k \theta_j X_{j, t-1} \right)
$$ Il faut bien noter que tous ces cas ne sont pas tous utilisables pour une même modélisation. En effet, on ne peut pas imposer une contrainte (ou une absence de contrainte) sur des paramètres qui n'ont pas été inclus dans le modèle $ARDL$ ou $UECM$ sous-jacent. Par exemple, si le modèle $ARDL$ n'inclue pas de tendance linéaire, il n'est pas possible d'utiliser les cas où on impose une tendance linéaire dans l'équation de court- ou long-terme puisque cette tendance n'existe pas dans le modélisation initiale.

### Bound F test / Bound t test

Les tests initiaux de cointégration ne fonctionnent généralement que si toutes les variables sont intégrées du même ordre ce qui dans un certain nombre de cas pratiques n'est pas réaliste. **Pesaran et al (2001)** ont développé deux tests statistique afin de tester s'il existe une relation de long-terme entre les variables en niveau et ce même si toutes les variables ne sont intégrées du même ordre.

Les valeurs critiques de ces tests ne sont pas standards. **Pesaran et al (2001)** ont tabulés des valeurs critiques lorsque toutes les variables sont intégrées d'ordre 0 et lorsque toutes les variables sont intégrées d'ordre 1. Cela donne un intervalle permettant de déterminer si un ensemble de variables $I(0)$ et $I(1)$ possèdent une relation de long-terme ou non.

Le premier test est un test de Wald (sorte d'équivalent au test de Fisher) qui va tester les coefficients associés aux variables en niveau sont tous conjointement différents de 0.

Ainsi à partir du modèle :

$$
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t
$$

On va tester (en fonction du cas de modèle $RECM$ choisi) :

-   **Cas 1, 3, 5**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq 0     
\end{array}
\right.
$$

-   **Cas 2 :**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = c_0 = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq c_0 \neq 0     
\end{array}
\right.
$$

-   **Cas 4**

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = \pi_1 = \dots = \pi_k = c_1 = 0  \\       
H_1 : \pi_y \neq \pi_1 \neq \dots \neq \pi_k \neq c_1 \neq 0     
\end{array}
\right.
$$

Sous l'hypothèse nulle, ne présentent pas de relation de long-terme en niveau. Elles ne sont pas cointégrées. On estime donc un $ARDL$ en première différence classique. Sous l'hypothèse alternative, il existe une relation de long-terme entre les variables en niveau. On peut donc estimer un modèle à correction d'erreur.

Une fois la statistique de test obtenue (non reportée ici) notée $F$, il faut la comparer aux valeurs critiques tabulés notées $F_{I(0)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(0)$ et $F_{I(1)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(1)$. Cela donne l'intervalle : $[F_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} F_{I(1)}]$. La règle de décision est la suivante :

-   Si $F < F_{I(0)}$ alors on ne peut pas rejeter $H_0$. Il semblerait qu'il n'existe pas de relation en niveau entre les variables peu importe que les variables soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées.

-   Si $F > F_{I(1)}$} alors on peut rejeter l'hypothèse nulle. Il semblerait qu'il existe une relation en niveau entre les variables peu importe quelles soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées

-   Si $F \in [F_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} F_{I(1)}]$ alors on ne peut pas tirer de conclusion à partir de ce test. Il faut regarder variable par variables leur ordre d'intégration avant de faire des tests d'inférence.

Le deuxième test proposé par **Pesaran et al (2001)** est un dérivé du test de Student qui va tester si la force de rappel de long-terme est significativement différente de 0 dans un modèle $UECM$.

Ainsi à partir du modèle :

$$ 
\Delta Y_t = c_0 + c_1t + \pi_y Y_{t-1} + \sum_{j=1}^k \pi_j X_{j,t-1} + \sum_{i=1}^{p-1}\psi_{y,i} \Delta Y_{t-i} + \sum_{j=1}^k \sum_{l=1}^{q_j - 1} \psi_{j,l} \Delta X_{j,t-l} + \sum_{j=1}^k \omega_j \Delta X_{j,t} + \varepsilon_t 
$$

On va tester :

$$
\left\{    
\begin{array}{ll}       
H_0 : \pi_y = 0  \\       
H_1 : \pi_y  \neq 0     
\end{array}
\right.
$$

Une fois la statistique de test obtenue (non reportée ici) notée $t$, il faut la comparer aux valeurs critiques tabulés notées $t_{I(0)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(0)$ et $t_{I(1)}$ pour la valeur critique obtenue lorsque toutes les variables sont $I(1)$. Cela donne l'intervalle : $[t_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} t_{I(1)}]$. La règle de décision est la suivante :

-   Si $t < t_{I(0)}$ alors on ne peut pas rejeter $H_0$. Il semblerait qu'il n'existe pas de force de rappel de long-terme. Ce qui semblerait indiquer l'absence de relation en niveau entre les variables peu importe que les variables soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées.

-   Si $t > t_{I(1)}$} alors on peut rejeter l'hypothèse nulle. Il semblerait qu'il existe une force de rappel de long-terme. Ce qui semblerait indiquer la présence de relation en niveau entre les variables peu importe quelles soient toutes $I(0)$, $I(1)$ ou mutuellement cointégrées

-   Si $t \in [t_{I(0)} \hspace{0.2cm}; \hspace{0.2cm} t_{I(1)}]$ alors on ne peut pas tirer de conclusion à partir de ce test. Il faut regarder variable par variables leur ordre d'intégration avant de faire des tests d'inférence.

# Code

## Setup

Charger les librairies et tous les objets de setup. Comme rien n'est chargé pour l'instant il faut :

-   Utiliser la syntaxe `here::here()` pour dire que l'on utilise la fonction `here()` du package `here`

-   Indiquer le chemin en entier jusqu'au script `setup.R`

```{r}
#| message: false
#| warning: false
# Exécute le script setup.R pour charger tous les éléments importants
source(here::here("02-codes", "utils", "setup.R"))
```

## Présentation des données

Nous voulons modéliser la monnaie réelle (agrégat M2) en fonction du revenu réel, du taux d'intérêt des obligations étatiques et du taux d'intérêt des dépôts bancaires au Danemark de 1974:Q1 à 1987:Q3.

Commençons par charger les données contenues dans l'objet `ARDL::denmark` et transformons le en objet de classe `tibble` pour plus de facilités à manipuler les données. (Si certaines fonctions imposent l'utilisation d'un objet TS, alors on le retransformera en temps voulu).

```{r}
# Visualiser à quoi ressemble l'objet denmark et l'enregister dans l'objet df_ts
df_ts <- 
  ARDL::denmark

df_ts |> head()
```

Il s'agit d'un objet de classe TS (Time Series) contenant 5 colonnes pour les différentes variables. Les dates sont indiquées en tantque noms des lignes. On peut vouloir transformer ce objet Time Series en un tibble afin d'avoir plus de facilité à manipuler les données ou pour les représenter graphiquement. Pour cela, le package `timetk` propose la fonction `tk_tbl()` pour transformer un objetv Time Series en tibble facilement, en conservant l'unité de temps si on le souhaite.

```{r}
df <- 
  df_ts |> 
  timetk::tk_tbl(
    rename_index = "date" # Rename the time variable "date"
  ) |> 
  print()
```

L'index temporel a bien été ajouté dans la variable "date" sous un format `yearqtr` indiquant un format trimestriel. Avec cette tibble on peut facilement utiliser le package `ggplot2` pour représenter graphiquement nos séries. Afin de toutes les mettre dans un seul graphique facilement, on va pivoter les données afin que toutes les valeurs soient dans une seule colonne et que le nom des variables soit dans une autre colonne.

```{r}
df |> 
  # Regrouper toutes les valeurs des variables d'intérêt dans une variable nommée "value" et le nom des variables associées dans une variable nommée "variable"
  pivot_longer(
    cols = !c(date),
    names_to = "variable",
    values_to = "value"
  ) |> 
  print() |> 
  # Faire un graphique en ligne
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  # Séparer les variables dans différents panels avec chaque axe qui leur est propre
  facet_wrap("variable", scales = "free")
```

On peut aussi tout simplement utiliser la fonction `plot()` sur l'objet time series, mais les possibilités de customisation sont plus faibles.

```{r}
# Avec un objet TS
plot(ARDL::denmark)
```

Il semblerait que les variables ne soient pas stationnaires. On peut le vérifier avec le test ADF par exemple. On va effectuer le test ADF sur chaque série. Pour éviter d'avoir à le faire manuellement pour chaque série, on peut faire une boucle `for` qui va nous permettre de boucler une opération sur un certain nombre d'éléments.

On peut cependant être plus efficace en utilisant le package `purrr`. La fonction `map()` permet d'appliquer une fonction sur chaque élément d'un vecteur ou d'une liste donné. Ainsi `map(x, f)` va appliquer la fonction `f` à chaque élément contenu dans `x`. La fonction `map2(x, y, f)` permet de faire la même chose sauf qu'il y a deux vecteurs simultanés `x` et `y` qui vont rentrer dans la fonction `f`. De base ces fonctions renvoient une liste. La fonction `map2_dfr()` permet simplement de rendre un tibble au lieu d'une liste.

```{r}
# N'exécuter que si le package "dobby" est installé
if (rlang::is_installed("dobby")) {
  # Appliquer le test ADF sur chaque série (on peut utiliser l'objet tibble ou bien l'objet TS sans distinction)
  # La fonction a besoin de 2 élements : la série et son nom
  purrr::map2_dfr(
    list(df$LRM, df$LRY, df$IBO, df$IDE), # Série à tester
    c("LRM", "LRY", "IBO", "IDE"), # Nom de la série
    # Créer une fonction prenant deux paramètres : série et name
    # Cette fonction exécuter la fonction adf_test_auto
    \(serie, name) dobby::adf_test_auto(
      serie, name, 
      lags = 20, 
      message = FALSE, 
      return_res = TRUE
    )
  )
} else {
  print("Le package 'dobby' n'est pas installé.")
}


```

Les tests ADF nous indiquent que les séries LRY et IBO ne semblent pas stationnaires. Leur inclusion dans un modèle utilisant les MCO peut résulter en une corrélation fallacieuse. Les coefficients associés ont de très fortes chances d'être biaisés fortement. Si le package `dobby` n'est pas installé, le test ADF peut se faire manuellement avec la fonction `urca::ur.df()`. Attention, les p.values indiquées ne sont pas les bonnes, et les valeurs critiques données par la fonction concernent le test joint ADF et pas le test individuel (que l'on préfère généralement).

## Régression statique

Dans un premier temps, modélison la relation (possiblement fallacieuse) suivante :

$$
\text{LRM}_t = \alpha + \beta_1 \text{LRY}_t + \beta_2 \text{IBO}_t + \beta_3 \text{IDE}_t +u_t
$$

```{r}
# Régression statique
summary(lm(LRM ~ LRY + IBO + IDE, data = df))
```

A partir du modèle statique, on peut voir que les variables LRY et IBO sont très fortement significatives et que le R² ajusté est de 0,9218 ce qui est extrêmement élevé et un signe de corrélation fallacieuse/de coefficients biaisés à cause de la non stationnarité.

Un moyen simple de prendre en compte la non stationnarité consiste à différencier les variables non-stationnaires avant de les inclure dans le modèle de régression. Pour cela on peut soit créer des variables dans le tibble `df`, soit on peut utiliser la fonction `dynlm::dynlm()` qui permet d'appliquer des opérateurs de séries temporelles.

```{r}
# Différencier les variables LRY et IBO
df_diff <- 
  df |> 
  # Modifier le dataframe df
  mutate(
    # Effectuer la même opération pour les variable sélectionnées
    across(
      .cols = c(LRY, IBO), # Sélectionner les variables LRY et IBO
      .fns = \(variable) variable - lag(variable), # Leur appliquer une fonction qui va faire la différence première
      .names = "diff_{.col}" # Créer une nouvelle variable qui va prendre le nom diff_ suivi du nom originel de la variable
    )
  ) |> 
  # Afficher le résultat
  print()
```

Le modèle estimé devient :

$$
\text{LRM}_t = \alpha + \beta_1 \Delta \text{LRY}_t + \beta_2 \Delta \text{IBO}_t + \beta_3 \text{IDE}_t +u_t
$$

Avec $\Delta X_t = X_t - X_{t-1}$.

```{r}
# Régression statique avec toutes les variables I(0)
reg_statique <- 
  lm(LRM ~ diff_LRY + diff_IBO + IDE, data = df_diff) |> 
  summary() |> 
  print()
```

On peut obtenir le même résultat en passant directement des opérateurs de série temporelle à la fonction `dynlm()`. L'opérateur `d(x,i)` permet d'appliquer une différence d'ordre $i$ à la variable $x_t$. $d(x, 1) = x_t - x_{t-1}$ ; $d(x, 2) = x_t - x_{t-2}$. Pour utiliser ces opérateurs temporels, il faut absolument passer comme argument des données de type TS (ou autre format de séries temporelles).

```{r}
# Utiliser des opérateurs de séries temporelles directement
dynlm::dynlm(
  LRM ~ d(LRY) + d(IBO) + IDE, # d() -> fait la différence première d'une série 
  data = df_ts # Obligé de transformer le dataframe en objet Time Series pour faire la différenciation
) |> 
  summary()
```

On peut voir (les deux méthodes donnant strictement le même résultat) que les variables en premières différences ne sont plus significatives tandis que la variable IDE devient significative. L'augmentation des taux d'intérêts bancaire fait diminuer fortement l'offre totale de monnaie M2 puisqu'il y a moins de crédits en circulation dans l'économie.

Un des point gros point d'attention lorsque l'on travaille avec des séries temporelles est le fait de savoir si toute l'autocorrélation des séries a bien été prise en compte. Sinon cela retire la propriété de variance minimale des estimateurs et nous indique également que notre modélisation est incomplète et n'arrive pas à correctement reproduire les caractéristique de notre série d'origine.

On utilise donc le test de Ljung-Box sur notre série des résidus.

```{r}
# N'exécuter que si le package dobby est installé
if (rlang::is_installed("dobby")) {
  dobby::check_univariate_autocorr(reg_statique$residuals, "résidus statiques", test_type = "Box-Pierce", return_output = FALSE)
} else {
  print("Le package 'dobby' n'est pas installé")
}

```

On peut voir que nos résidus sont fortement autocorrélés. Cela n'est pas surprenant puisque nous n'avons inclue aucune dynamique dans notre régression. Notre modélisation est donc incomplète.

## Régression avec variable dépendante retardée

Pour prendre en compte plus de dynamique, on peut rajouter des retards dans les variables explicatives ou bien ajouter des retards de la variable dépendante. Pour ce faire, on peut soit créer les variables dans un dataframe, soit on utilise la fonction `dynlm()`. L'opérateur `L(x,i)` permet d'indiquer que l'on prend la variable $x$ retardée de $i$ périodes. $L(x,2) = x_{t-2}$.

Le modèle estimé devient :

$$
\text{LRM}_t = \alpha + \beta_0 \text{LRM}_{t-1} + \beta_1 \Delta \text{LRY}_t + \beta_2 \Delta \text{IBO}_t + \beta_3 \text{IDE}_t +u_t
$$

```{r}
# Régression avec variable dépendante retardée
reg_dep_retard <- 
  dynlm::dynlm(
  LRM ~ L(LRM) + d(LRY) + d(IBO) + IDE,
  data = df_ts
) |> 
  summary() |> 
  print()
```

On peut re-tester l'autocorrélation des résidus de notre modèle.

```{r}
# N'exécuter que si le package 'dobby' est installé
if (rlang::is_installed("dobby")) {
  dobby::check_univariate_autocorr(reg_dep_retard$residuals, "résidus dépendante retardée", test_type = "Box-Pierce", return_output = FALSE)
} else {
  print("Le package 'dobby' n'est pas installé")
}
```

Outre le R² ajusté très élevé qui est suspect, on peut voir que l'inclusion de la variable dépendante retardée permet de prendre en compte d'avantage d'autocorrélation à l'ordre 1. En revanche on modélise très mal l'autocorrélation de long-terme. Il s'agit d'un indice indiquant que l'on n'a pas pris en compte assez de retards dans notre modélisation (ou qu'il nous manque des variables explicatives).

## Modélisation ARDL

La fonction `dynlm()` est très puissante mais sa syntaxe peut vite devenir encombrante lorsque les retards et les différences s’enchaînent. Le package `ARDL` permet d'obtenir les mêmes résultats mais avec une syntaxe bien moins volumineuse. On peut utiliser la même syntaxe que pour la fonction `dynlm()` mais au lieu d'avoir à indiquer chaque retard pour chaque variable, on indique simplement le nombre de retards que l'on souhaite inclure dans la modélisation pour chaque variable via le paramètre `order`.

On peut passer comme argument de données un tibble ou un objet de séries temporelles. Certaines fonctionnalités vont marcher avec un type d'objet et pas l'autre. Par exemple la création et représentation de l'équation de long-terme nécessite un objet de Série Temporelle pour fonctionner correctement. On va donc choisir d'utiliser un objet de série temporelle dès qu'on le peut.

Le package `timetk` permet encore une fois de facilement passer d'un tibble à un objet TS avec la fonction `tk_ts()`.

```{r}
# Transformer df_diff en objet de Série temporelle
df_diff_ts <- 
  df_diff |> 
  timetk::tk_ts(
    select = !date, # La variable date doit être retirée
    start = min(df_diff$date), # On indique que notre série temporelle commence à la première date
    frequency = 4 # On indique que l'on se trouve avec des données trimestrielles : 4 périodes par années
  )

# Afficher les premières observations
df_diff_ts |> 
  head()
```

On va maintenant utiliser le package `ARDL` pour modéliser notre série LRM avec une variable dépendante retardée.

```{r}
# Modélisation avec variable dépendante retardée
ARDL::ardl(
  LRM ~ diff_LRY + diff_IBO + IDE, # variables à utiliser
  data = df_diff_ts, # Dataframe ou objet TS
  order = c(1, 0, 0, 0) # Nombre de retards à inclure pour chaque variable. La première est la variable dépendante
) |>
  summary()
```

On peut ajouter facilement de nouveaux retards sur nos variables. Ici nous allons estimer un $ARDL(1, 2, 3, 1)$ pour l'exemple.

```{r}
# Ajouter des retards aux variables explicatives
ARDL::ardl(
  LRM ~ diff_LRY + diff_IBO + IDE, # variables à utiliser
  data = df_diff_ts, # Dataframe ou objet TS
  order = c(1, 2, 3, 1) # Nombre de retards à inclure
) |>
  summary()
```

## Procédure de Pesaran et al (2001)

Comme nous l'avons vu, notre jeu de données est caractérisé par des variables $I(0)$ stationnaires et des variables $I(1)$ non-stationnaires. Cette disparité empêche d'utiliser la procédure d'Engle et Granger pour tester la cointégration de nos variables. Actuellement, on ne sait pas s'il existe une relation entre les niveaux de nos variables et s'il est donc correct de modéliser nos séries selon une telle relation ou selon un modèle à correction d'erreur. La meilleure chose que l'on pouvait faire était donc de différencier nos variables $I(1)$ afin d'obtenir des variables $I(0)$ et les inclure dans la modélisation.

Cependant, cela est limitant car ce qui nous intéresse est le niveau des taux d'intérêts et pas leur taux de croissance. Pour pallier ce problème, on peut utiliser la méthode de Pesaran et al (2001) afin de tester s'il existe une relation entre les niveaux de nos variables ou non.

### Estimation de l'ARDL en niveau

La première étape consiste à trouver et estimer le meilleur ARDL entre les niveaux de nos variables. Pour cela, on peut tester différentes spécifications, utiliser les tests sur les résidus ou bien utiliser les critères d'information. La fonction `ARDL::auto.ardl()` permet de trouver le meilleur ARDL en fonction du critère d'information choisi.

```{r}
# Tester "tous" les modèles et les classer en fonction de leur AIC 
models <- 
  ARDL::auto_ardl(
    LRM ~ LRY + IBO + IDE, # Variables à utiliser
    data = df_ts, # Source des variables
    max_order = 5, # Nombre de retards maximum autorisé pour les variables
    selection = "AIC" # critère d'information à utiliser
  )

# Afficher le classement des modèles
models$top_orders |> 
  print()
```

On peut voir que le modèle ARDL qui minimise le critère AIC (pondération entre l'explication du modèle et le nombre de paramètres à estimer) est le modèle $ARDL(3,1,3,2)$. Le modèle avec simplement un retard sur la variable retardée est le modèle numéro 16.

Ces modèles étant déjà estimés, on peut obtenir les résultats associés :

```{r}
# Enregistrer le meilleur modèle dans une variables
ardl_3132 <- models$best_model

# Afficher le résumé des résultats
ardl_3132 |> 
  summary()
```

### Tester la sphéricité des erreurs

On peut, tout comme un modèle linéaire en coupe transversale, tester si les erreurs sont sphériques. Pour rappel, en série temporelle, les erreurs sont dites sphériques si leur variance est constante et si elles ne sont pas autocorrélées dans le temps. On peut utiliser les tests de Breush-Pagan et de White pour tester si la variance des résidus est constante.

```{r}
# Breusch-Pagan test
lmtest::bptest(ardl_3132)

# White test
whitestrap::white_test(ardl_3132)
```

Il semblerait que l'on ne puisse pas rejeter l’hypothèse nulle d'homoscédasticité à partir de ces tests. On peut également tester pour la présence d'effets ARCH, c'est à dire de variance conditionnellement hétéroscédastique avec le test ARCH.

```{r}
fDMA::archtest(as.numeric(ardl_3132$residuals**2), lag = 20)
```

Une fois de plus, on ne peut pas rejeter l'hypothèse nulle d'homoscédasticité conditionnelle. Il semblerait que nos résidus aient une variance constante.

Pour tester l'autocorrélation, on peut utiliser les tests de Ljung-Box ou Box-Pierce sur la série des résidus. Une fonction "wrap-up" est présente dans le package `dobby`. Dans le cas où celui-ci ne fonctionne pas (pour des raisons d'installation), on peut utiliser la fonction `stats::Box.test()` qui est la fonction utilisée par le package `dobby`.

```{r}
# N'exécuter que si le package dobby est installé
if (rlang::is_installed("dobby")) {
  # Ljung-Box test for lags 1 and 20 with dobby package
  dobby::check_univariate_autocorr(ardl_3132$residuals, "résidus ARDL(3132)", return_output = FALSE)
} else {
  print("Le package 'dobby' n'est pas installé")
}
```

```{r}
# Ljung-Box test for lag 1
stats::Box.test(ardl_3132$residuals, lag = 1, type = "Ljung-Box")
```

On ne peut pas rejeter l'hypothèse nulle de non-autocorrélation des résidus. Il semblerait que nos erreurs soient sphériques. La variance de nos estimateurs devrait donc être minimale. Il semblerait également que l'on ait correctement réussi à prendre en compte la dynamique de notre variable dépendante.

### Corriger l'hétéroscédasticité

Si jamais on soupçonne la non-sphéricité des erreurs d'être un problème, on peut utiliser des écarts-types robustes à l'hétéroscédasticité et/ou l'autocorrélation.

```{r}
# Ecrats-types robustes à l'hétéroscédasticité à la White
ardl_3132 |> 
  lmtest::coeftest(vcov. = car::hccm)
```

```{r}
# Ecarts-types robustes à l'hétéroscédasticité et l'autocorrélation à la Newey-West
ardl_3132 |> 
  lmtest::coeftest(vcov. = sandwich::vcovHAC)
```

```{r}
# Ecarts-types robustes à l'hétéroscédasticité et l'autocorrélation à la Newey-West avec pré-mise en bruit blanc des résidus via un VAR à la Andrews & Monahan (1992)
ardl_3132 |> 
  coeftest(vcov. = sandwich::NeweyWest)
```

Savoir si et quel type de correction utiliser est un problème empirique dont la solution dépend du problème rencontré. Une bonne pratique peut cependant être de tester la robustesse de ses résultats avec différentes corrections afin de voir l'impact de chacune d'entre elle. Ici, il semblerait que les différentes corrections apportées ne changent pas les résultats de significativité de manière importante.

Rappel que nous ne savons pas encore si notre ARDL en niveau est valide comme nous n'avons pas encore testé s'il existe une relation entre les niveaux de nos variables.

### Tester la présence d'une relation en niveau

Il semble que la capacité explicative de ce modèle soit très élevée. Cependant à ce stade on ne sait pas encore s'il existe une relation entre les variables en niveau. Pour cela on va estimer le modèle $UECM$ à partir du meilleur $ARDL$ en niveau (c'est à dire qu'on va choisir le même nombre de retards pour chaque variable en différence).

```{r}
# Estimer le modèle UECM à partir du meilleur ARDL en niveau trouvé
uecm_3132 <- ARDL::uecm(ardl_3132)

# Afficher les résultats
uecm_3132 |> 
  summary()
```

Le modèle $UECM$ nous permet d'utiliser le Bound F-test de Pesaran et al (2001) qui va tester l'hypothèse nulle de non significativité conjointe des coefficients des variables en niveau. Pour cela on utilise la fonction `ARDl::bound_f_test()` qui va prendre comme argument, entre autre, un ARDL ou UECM (le modèle ARDL sera automatiquement transformé en UECM) et le "cas" indiquant si la constante et/ou la tendance doivent participer à l'équation de court- ou long-terme. Le paramètre `alpha` permet d'indiquer le niveau de significativité des valeurs critiques. En cas d'absence, seule la p.value est renvoyée.

```{r}
# Bound F test dans lequel la constante participe dans l'équation de long-terme. 
# Valeurs critiques pour un niveau de significativité de 1%
res_f_test <- 
  ARDL::bounds_f_test(
    uecm_3132, # UECM
    case = 2, # Constante dans l'équation de long-terme. Pas de tendance
    alpha = 0.01 # Niveau de significativité de 1%
  )

# Afficher le résultat et les valeurs critiques
res_f_test$tab
```

Si l'on considère la non-sphéricité des erreurs peut être un problème (ce qui ne semble pas être notre cas), on peut utiliser une matrice de variance-covariance des estimateurs robustes à ces problèmes avec le paramètre `vcov_matrix`.

```{r}
# Bound F-test avec la correction HAC de la matrice de variance-covariance des estimateurs
ARDL::bounds_f_test(
  uecm_3132, # UECM
  case = 2, # Constante dans l'équation de long-terme. Pas de tendance
  alpha = 0.01, # Niveau de significativité de 1%
  vcov_matrix	= sandwich::vcovHAC(uecm_3132) # Correction HAC
)[["tab"]]
```

Le Bound F-test sans correction de la matrice de variance-covariance nous donne l'intervalle suivant : $[3.67 ; 4.61]$. La valeur de la statistique calculée est $5.12$. Cette statistique est supérieure à la valeur critique supérieure $I(1)$. On rejette donc l'hypothèse nulle de non-significativité conjointe des coefficients en niveau au seuil de 1%. Il semblerait qu'il existe une relation de entre les niveaux des variables sélectionnées. La correction HAC ne change pas notre conclusion.

```{r}
# Ne pas indiquer les valeurs critiques. 
ARDL::bounds_f_test(
    uecm_3132, # UECM
    case = 2, # Constante dans l'équation de long-terme. Pas de tendance
  )
```

On va tester la significativité du coefficient de réversion à la moyenne (donné par le coefficient de la variable dépendante retardée en niveau) avec le Bound t-test de Pesaran et al (2001). Attention il ne faut pas regarder le test de Student classique cela est une erreur. Attention également, le Bound t-test n'est applicable que dans les cas 1, 3, 5...

```{r}
res_t_test <- 
  ARDL::bounds_t_test(
    uecm_3132, 
    case = 3, # La tendance passe dans l'équation de court-terme
    alpha = 0.01 # Seuil de significativité à 1%
  )

res_t_test$tab
```

On peut encore une fois appliquer une correction à la matrice de variance-covariance si nécessaire.

```{r}
ARDL::bounds_t_test(
  uecm_3132, 
  case = 3, # La tendance passe dans l'équation de court-terme
  alpha = 0.01, # Seuil de significativité à 1%
  vcov_matrix	= sandwich::vcovHAC(uecm_3132) # Correction HAC
)[["tab"]]
```

L'intervalle en valeur absolue est donnée par : $[3.44;4.38]$ . La statistique en valeur absolue est $4.55$. Elle est supérieure à la borne supérieure $I(1)$. On rejette donc l'hypothèse nulle de nullité de réversion à la moyenne. Il semblerait encore une fois qu'il existe une relation en niveau entre nos variables et que la correction HAC ne change pas nos résultats.

```{r}
# Simplement afficher le résultat du test avec la p.value
ARDL::bounds_t_test(
    uecm_3132, 
    case = 3, # La tendance passe dans l'équation de court-terme
)
```

### Multiplicateurs

Au vu des tests qui sont concordants, il semblerait bien qu'il existe une relation entre les niveaux de nos variables bien quelles ne soient pas toutes intégrées du même ordre. On peut donc s'intéresser à l'interprétation des coefficients. Cependant, il peut être difficile de trouver l'interprétation à donner aux coefficients à cause de la dynamique du modèle et des nombreux retards inclus. Pour cela, on va calculer et interpréter les coefficients multiplicateurs de court- et long-terme avec la fonction `ARDL::multipliers()`. L'argument `type` permet d'indiquer le type de multiplicateur que l'on souhaite obtenir : "sr" pour le court-terme ; "lr" ou rien pour le long-terme ; un chiffre pour les multiplicateurs Delays et Interim.

```{r}
# Multiplicateurs de court-terme
ARDL::multipliers(
  ardl_3132, 
  type = "sr" # Indiquer que l'on souhaite les multiplicateurs de court-terme
)
```

Une augmentation temporaire d'1% (les variables sont en log) du revenu réel augmente l'offre de monnaie réelle de 0,67%. Cette augmentation est significativement différente de 0. L'augmentation du revenu réel conduit à d'avantage d'emprunt et un plus grand besoin de monnaie ce qui explique l'augmentation. Cependant, l'effet est inférieur à l'unité. L'offre de monnaie augmente moins fortement que l'augmentation du revenu.

A l'inverse lorsque les taux d'intérêts augmentent de manière temporaire de 1%, l'offre de monnaie réelle diminue de 1,07%. Une augmentation des taux rend les crédits plus onéreux et risqués ce qui diminue le nombre de crédits accordés et donc la création monétaire. Il semblerait que l'offre de monnaie et les taux d'intérêts évoluent de concert.

En revanche, les taux de dépôts bancaires n'ont pas l'air d'avoir un effet à court terme sur l'offre de monnaie réelle puisque le coefficient n'est pas significativement différent de 0.

```{r}
# Multiplicateurs de long-terme
ARDL::multipliers(ardl_3132) # Si aucun type n'est indiqué alors, il s'agit du long-terme
```

Une augmentation permanente d'1% du revenu réel va entraîner une hausse de presque 1% de l'offre de monnaie réelle. L'effet est plus élevé que dans le court terme car la dynamique du modèle permet à l'augmentation de se distribuer dans le temps, augmentant ainsi l'effet final comparé à l'effet à l'impact. Il en va de même pour l'effet des taux d'intérêts qui font diminuer de 4,5% l'offre de monnaie réelle suite à une augmentation permanente de 1%. Il s'agit d'un effet extrêmement élevé. On peut expliquer cela par le fait que les taux d'intérêts ne sont généralement pas augmentés de manière permanente. Les augmentation (et donc l'écart à la valeur d'équilibre) sont généralement temporaires afin de limiter l'inflation.

Dans le long-terme, le taux de dépôt des banques devient significatif. On peut argumenter qu'il faut du temps pour que le taux de dépôt se traduise par une augmentation des dépôts et donc une plus forte création de monnaie.

On peut encore une fois corriger, si nécessaire, la matrice de variance-covariance des estimateurs en cas de problème de sphéricité des erreurs :

```{r}
ARDL::multipliers(
  ardl_3132,
  vcov_matrix	= sandwich::vcovHAC(ardl_3132)
)

```

Si l'on s'intéresse à un effet dynamique et pas simplement à un impact immédiat, on peut regarder les multiplicateurs Delays qui indiquent la variation à une période $t+S$ suite à une variation temporaire en $t$. Ici on va regarder jusqu'à $t+15$.

```{r}
# Calculer les multiplicateurs Delays jusqu'à 15 périodes
ARDL::multipliers(
  ardl_3132, 
  type = 15, # Nombre de Delays à calculer
  se = TRUE # Indiquer si l'on souhaite obtenir les écarts-types
) |> 
  ARDL::plot_delay(interval = 0.95) # Représenter graphiquement les impacts temporaires avec un intervalle de confiance de 95%
```

Notre argument concernant le temps nécessaire aux taux de dépôt bancaire pour faire effet semble se valider puisqu'on peut remarquer que l'effet d'une augmentation temporaire ne se manifeste qu'en $t+2$ puis $t+4$. A l'inverse, on peut voir que l'effet d'une augmentation temporaire des taux d'intérêts va avoir un effet direct sur la quasi totalité des périodes, bien que celui-ci diminue. Rappel que IBO est une variable caractérisée par une variable unitaire. Les chocs qu'elle reçoit sont donc persistants ce qui peut expliquer cet effet persistant.

### Relation de long-terme et vitesse d'ajustement

Il est possible à partir des multiplicateurs de long-terme de construire l'équation de cointégration. C'est à dire l'équation de long-terme, d'équilibre de l'offre de monnaie.

```{r}
# Calculer l'éuqation de cointégration (de long-terme de la variable LRM)
ce <- 
  ARDL::coint_eq(
    ardl_3132, # Modèle utiliser pour calculer l'équation de cointégration. Doit être un objet TS sinon ça devient bizarre
    case = 2 # Constante dans l'équation de long-terme et pas de tendance
  )

# Représenter graphiquement la relation de cointégration contre les vraies valeurs de LRM
ARDL::plot_lr(ardl_3132, coint_eq = ce, show.legend = TRUE)
```

On peut voir que sur les dernières années disponibles, l'offre de monnaie réelle a généralement été inférieure à son niveau de long-terme, impliquant des chocs lui faisant dévier de son équilibre. On peut retrouver la vitesse d'ajustement à ces chocs à partir du modèle $UECM$ ou $RECM$. Ce dernier étant plus parcimonieux, les estimateurs devraient être légèrement plus précis.

```{r}
# Estimer le modèle RECM
recm_3132 <- ARDL::recm(uecm_3132, case = 2)
summary(recm_3132)
```

La vitesse d'ajustement est donnée par le coefficient du terme $ect$. Elle est ici de $-0,41685$ ce qui indiquerait un effet d'ajustement plutôt modéré. On peut calculer le temps que met l'offre de monnaie à revenir à sa situation d'équilibre suit à une variation : $\frac{1}{0.41685} \approx 2.4$. Il faut environ 2.4 périodes de temps (donc deux trimestres et demi) pour que l'offre de monnaie revienne à son niveau d'équilibre suite à un écart.
